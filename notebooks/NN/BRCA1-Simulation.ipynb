{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial neural nework training\n",
    "Following parameters described in Evo2 preprint (section 4.3.16 BRCA supervised classiication).\n",
    "\n",
    "* Training method: input vector derived from concatenation of the Reference (8192 elements) and Variant vectors (8192 elements).\n",
    "\n",
    "* Training samples: 4800 by 16384 embeddings, one binary output (0 or 1)\n",
    "\t* 2400 items are labelled 0, which have Variant vector  0.1%, 0.2% or 0.3% of elements different from Reference vector.\n",
    "    * 2400 items are labelled 1, which have Variant vector 2%, 3% or 4% of elements different from Reference vector.\n",
    "\n",
    "* Testing samples: 600 by 16384, one binary output (0 or 1)\n",
    "    * 300 items are labelled 0, which have Variant vector 0.1%, 0.2% or 0.3% of elements different from Reference vector.\n",
    "    * 300 items are labelled 1, which have Variant vector 2%, 3% or 4% of elements different from Reference vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 16385)\n",
      "new_train.shape =  (4800, 16385)\n",
      "(600, 16385)\n",
      "new_test.shape =  (600, 16385)\n"
     ]
    }
   ],
   "source": [
    "# READ IN CSV\n",
    "DIR = \"/mnt/nfs/rigenenfs/shared_resources/biobanks/UKBIOBANK/pangk/evo2/NN\"\n",
    "out_train = os.path.join(DIR, \"train_vectors2.csv\")\n",
    "out_test = os.path.join(DIR, \"test_vectors2.csv\")\n",
    "\n",
    "new_train_values = np.loadtxt(out_train,delimiter=\",\")\n",
    "print(new_train_values.shape)\n",
    "new_train = new_train_values.reshape((4800,16385))\n",
    "print('new_train.shape = ',new_train.shape) # new_train.shape =  (4800, 16385)\n",
    "\n",
    "new_test_values = np.loadtxt(out_test,delimiter=\",\")\n",
    "print(new_test_values.shape)\n",
    "new_test = new_test_values.reshape((600,16385))\n",
    "print('new_test.shape = ',new_test.shape) # new_test.shape =  (600, 16385)\n",
    "\n",
    "print(new_train[:5,:5])  # Display first 5 rows and columns\n",
    "print(new_test[:5,:5])  # Display first 5 rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 16384)\n",
      "(4800,)\n",
      "(4800, 1)\n",
      "(600, 16384)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "# 16384 inputs; one binary output; 1500 rows/training cases\n",
    "# 8192*0.04 = 327.8 elements ----> multiply by 1.4\n",
    "# take absolute difference of the two vectors, sum it up, then divide by a constant to scale it down to between 0 and 1\n",
    "# input dimension: 16,385 \n",
    "\n",
    "train_x = new_train[:,0:16384] \n",
    "train_y = new_train[:,16384]\n",
    "print(train_x.shape) # (4800, 16384)\n",
    "print(train_y.shape) # (4800,)\n",
    "\n",
    "df_y = pd.DataFrame({\n",
    "    \"output_vector\": train_y\n",
    "})\n",
    "print(df_y.shape) # (4800, 1)\n",
    "df_y.head()\n",
    "\n",
    "#===================\n",
    "test_x = new_test[:,0:16384] \n",
    "test_y = new_test[:,16384]\n",
    "print(test_x.shape) # (600, 16384)\n",
    "print(test_y.shape) # (600,)\n",
    "\n",
    "# -------------- Scaling ------------------\n",
    "train_x = train_x*100 # scaling the elements \n",
    "print(np.max(train_x))\n",
    "test_x = test_x*100 # scaling the elements\n",
    "print(np.max(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "# Compute Binary Cross-Entropy using NumPy, closer to 0 is better, 0 means the predicted vector and test vector is exactly the same\n",
    "def binary_cross_entropy_np(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates Binary Cross-Entropy loss for multiple samples using NumPy.\n",
    "    y_true: NumPy array of actual labels (0s and 1s)\n",
    "    y_pred: NumPy array of predicted probabilities (between 0 and 1)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # Small value to prevent log(0), reference vector (0 or 1) needs to compare with positive. Ensure the probability is within 0 or 1 (feasible calc.)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "# Compute Binary Cross-Entropy using own calculaton for checking\n",
    "def binary_cross_entropy_check(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Objective: Calculates Binary Cross-Entropy loss for multiple samples using NumPy.\n",
    "    y_true: NumPy array of actual labels (0s and 1s)\n",
    "    y_pred: NumPy array of predicted probabilities (between 0 and 1)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # Small value to prevent log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "    N=len(y_true)\n",
    "    BCE_sum=0\n",
    "    for i in range(N): # loop through each element \n",
    "        BCE_sum = BCE_sum + y_true[i]*np.log(y_pred[i]) + (1-y_true[i])*np.log((1-y_pred[i])) \n",
    "    BCE = (-1*BCE_sum)/N\n",
    "    return BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start ANN training\n",
    "The results from the previous ANN using MLPRegressor is very poor !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 512)               8389120   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 8,461,633\n",
      "Trainable params: 8,460,289\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def baseline_model(): # 3 hiddenlayers, 512, 128, 32 hidden units (neurons); 8389120   parameters\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_dim = 16384)) # hidden layer 1 (512 neurons)\n",
    "    model.add(BatchNormalization()) # \n",
    "    model.add(Dropout(0.3)) #\n",
    "    model.add(Dense(128, activation='relu')) # hidden layer 2\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu')) # hidden layer 3\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid')) # output layer for binary classification | linear alternatre\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def baseline_model2(): # 2 hidden layers, 32 hidden units --> perhaps helps reduce overfitting and increase model performance?\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_dim = 16384)) # 32 num parameters\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid')) # output layer for binary classification\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "print('train_x.shape is ', train_x.shape) # (4800, 16384)\n",
    "\n",
    "# Build model\n",
    "ANN_model = baseline_model() # (16384 * 512) + 512 = 8,389,120 parameters\n",
    "ANN_model.summary()\n",
    "\n",
    "# Dropout step has NO \"parameters\". dropout(0.3) means 30% of the neurons are randomly set to 0 during training, not used in inference. Drop 30% of the interconnecting links/weights.\n",
    "# A technique to help prevent overfitting or memorizing the training data. Can experiment with this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "68/68 - 3s - loss: 0.8261 - accuracy: 0.5227 - val_loss: 0.7882 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "68/68 - 2s - loss: 0.4402 - accuracy: 0.8169 - val_loss: 0.9573 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "68/68 - 2s - loss: 0.2257 - accuracy: 0.9370 - val_loss: 1.6718 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "68/68 - 2s - loss: 0.1229 - accuracy: 0.9720 - val_loss: 2.5937 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "68/68 - 2s - loss: 0.0788 - accuracy: 0.9801 - val_loss: 3.6584 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "68/68 - 2s - loss: 0.0533 - accuracy: 0.9900 - val_loss: 3.9080 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "68/68 - 2s - loss: 0.0352 - accuracy: 0.9944 - val_loss: 4.4451 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "68/68 - 2s - loss: 0.0298 - accuracy: 0.9951 - val_loss: 4.3574 - val_accuracy: 0.0083\n",
      "Epoch 9/50\n",
      "68/68 - 2s - loss: 0.0295 - accuracy: 0.9931 - val_loss: 3.7646 - val_accuracy: 0.1000\n",
      "Epoch 10/50\n",
      "68/68 - 2s - loss: 0.0207 - accuracy: 0.9961 - val_loss: 4.1954 - val_accuracy: 0.1167\n",
      "Epoch 11/50\n",
      "68/68 - 2s - loss: 0.0242 - accuracy: 0.9940 - val_loss: 3.7339 - val_accuracy: 0.2250\n",
      "Epoch 12/50\n",
      "68/68 - 2s - loss: 0.0238 - accuracy: 0.9940 - val_loss: 1.9657 - val_accuracy: 0.5042\n",
      "Epoch 13/50\n",
      "68/68 - 2s - loss: 0.0306 - accuracy: 0.9896 - val_loss: 2.5291 - val_accuracy: 0.4000\n",
      "Epoch 14/50\n",
      "68/68 - 2s - loss: 0.0379 - accuracy: 0.9891 - val_loss: 2.3962 - val_accuracy: 0.4083\n",
      "Epoch 15/50\n",
      "68/68 - 2s - loss: 0.0452 - accuracy: 0.9854 - val_loss: 1.8265 - val_accuracy: 0.4792\n",
      "Epoch 16/50\n",
      "68/68 - 2s - loss: 0.0504 - accuracy: 0.9822 - val_loss: 2.2792 - val_accuracy: 0.4479\n",
      "Epoch 17/50\n",
      "68/68 - 2s - loss: 0.0555 - accuracy: 0.9806 - val_loss: 2.3915 - val_accuracy: 0.4146\n",
      "Epoch 18/50\n",
      "68/68 - 2s - loss: 0.0522 - accuracy: 0.9815 - val_loss: 1.2335 - val_accuracy: 0.6812\n",
      "Epoch 19/50\n",
      "68/68 - 2s - loss: 0.0550 - accuracy: 0.9787 - val_loss: 2.0087 - val_accuracy: 0.5458\n",
      "Epoch 20/50\n",
      "68/68 - 2s - loss: 0.0521 - accuracy: 0.9826 - val_loss: 3.0998 - val_accuracy: 0.4187\n",
      "Epoch 21/50\n",
      "68/68 - 2s - loss: 0.0393 - accuracy: 0.9847 - val_loss: 4.3631 - val_accuracy: 0.2479\n",
      "Epoch 22/50\n",
      "68/68 - 2s - loss: 0.0252 - accuracy: 0.9935 - val_loss: 2.2791 - val_accuracy: 0.5333\n",
      "Epoch 23/50\n",
      "68/68 - 2s - loss: 0.0291 - accuracy: 0.9877 - val_loss: 3.7441 - val_accuracy: 0.3375\n",
      "Epoch 24/50\n",
      "68/68 - 2s - loss: 0.0200 - accuracy: 0.9937 - val_loss: 2.6954 - val_accuracy: 0.4833\n",
      "Epoch 25/50\n",
      "68/68 - 2s - loss: 0.0220 - accuracy: 0.9924 - val_loss: 1.9103 - val_accuracy: 0.6104\n",
      "Epoch 26/50\n",
      "68/68 - 2s - loss: 0.0241 - accuracy: 0.9919 - val_loss: 2.7887 - val_accuracy: 0.4812\n",
      "Epoch 27/50\n",
      "68/68 - 2s - loss: 0.0180 - accuracy: 0.9942 - val_loss: 3.7920 - val_accuracy: 0.3729\n",
      "Epoch 28/50\n",
      "68/68 - 2s - loss: 0.0241 - accuracy: 0.9894 - val_loss: 2.8798 - val_accuracy: 0.4229\n",
      "Epoch 29/50\n",
      "68/68 - 2s - loss: 0.0259 - accuracy: 0.9910 - val_loss: 3.7473 - val_accuracy: 0.3583\n",
      "Epoch 30/50\n",
      "68/68 - 2s - loss: 0.0238 - accuracy: 0.9914 - val_loss: 4.6225 - val_accuracy: 0.2667\n",
      "Epoch 31/50\n",
      "68/68 - 2s - loss: 0.0280 - accuracy: 0.9903 - val_loss: 3.3557 - val_accuracy: 0.4104\n",
      "Epoch 32/50\n",
      "68/68 - 2s - loss: 0.0255 - accuracy: 0.9919 - val_loss: 3.7717 - val_accuracy: 0.3521\n",
      "Epoch 33/50\n",
      "68/68 - 2s - loss: 0.0222 - accuracy: 0.9917 - val_loss: 3.0878 - val_accuracy: 0.4479\n",
      "Epoch 34/50\n",
      "68/68 - 2s - loss: 0.0271 - accuracy: 0.9914 - val_loss: 5.3720 - val_accuracy: 0.2146\n",
      "Epoch 35/50\n",
      "68/68 - 2s - loss: 0.0169 - accuracy: 0.9951 - val_loss: 5.2346 - val_accuracy: 0.2438\n",
      "Epoch 36/50\n",
      "68/68 - 2s - loss: 0.0201 - accuracy: 0.9928 - val_loss: 4.8209 - val_accuracy: 0.2604\n",
      "Epoch 37/50\n",
      "68/68 - 2s - loss: 0.0142 - accuracy: 0.9937 - val_loss: 4.8401 - val_accuracy: 0.2688\n",
      "Epoch 38/50\n",
      "68/68 - 2s - loss: 0.0153 - accuracy: 0.9947 - val_loss: 4.4129 - val_accuracy: 0.2979\n",
      "Epoch 39/50\n",
      "68/68 - 2s - loss: 0.0178 - accuracy: 0.9940 - val_loss: 4.1845 - val_accuracy: 0.3375\n",
      "Epoch 40/50\n",
      "68/68 - 2s - loss: 0.0181 - accuracy: 0.9933 - val_loss: 3.7549 - val_accuracy: 0.3583\n",
      "Epoch 41/50\n",
      "68/68 - 2s - loss: 0.0189 - accuracy: 0.9926 - val_loss: 4.4927 - val_accuracy: 0.2875\n",
      "Epoch 42/50\n",
      "68/68 - 2s - loss: 0.0180 - accuracy: 0.9935 - val_loss: 4.7158 - val_accuracy: 0.3021\n",
      "Epoch 43/50\n",
      "68/68 - 2s - loss: 0.0189 - accuracy: 0.9928 - val_loss: 4.0713 - val_accuracy: 0.3500\n",
      "Epoch 44/50\n",
      "68/68 - 2s - loss: 0.0263 - accuracy: 0.9912 - val_loss: 4.2914 - val_accuracy: 0.3354\n",
      "Epoch 45/50\n",
      "68/68 - 2s - loss: 0.0278 - accuracy: 0.9914 - val_loss: 2.6059 - val_accuracy: 0.5354\n",
      "Epoch 46/50\n",
      "68/68 - 2s - loss: 0.0242 - accuracy: 0.9917 - val_loss: 4.8749 - val_accuracy: 0.2562\n",
      "Epoch 47/50\n",
      "68/68 - 2s - loss: 0.0159 - accuracy: 0.9947 - val_loss: 3.7674 - val_accuracy: 0.3833\n",
      "Epoch 48/50\n",
      "68/68 - 2s - loss: 0.0196 - accuracy: 0.9931 - val_loss: 5.6175 - val_accuracy: 0.2542\n",
      "Epoch 49/50\n",
      "68/68 - 2s - loss: 0.0172 - accuracy: 0.9935 - val_loss: 3.9403 - val_accuracy: 0.3896\n",
      "Epoch 50/50\n",
      "68/68 - 2s - loss: 0.0119 - accuracy: 0.9961 - val_loss: 4.1787 - val_accuracy: 0.4042\n",
      "Execution time:  87.8541247844696\n",
      "19/19 - 0s - loss: 3.4850 - accuracy: 0.4783\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "start_time = time.time()\n",
    "history = ANN_model.fit(train_x, train_y, epochs=50, batch_size=64,validation_split=0.1,verbose = 2)\n",
    "end_time = time.time()\n",
    "exe_time = end_time - start_time\n",
    "print(\"Execution time: \", exe_time)\n",
    "scores = ANN_model.evaluate(test_x,test_y,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 16384)\n",
      "(600,)\n",
      "(600, 1)\n",
      "** CHECK Average BCE Loss for multiple samples: [nan]\n",
      "Accuracy Score: 0.47833333333333333\n",
      "error0to1 =  143 ; label is 0\n",
      "error1to0 =  170 ; label is 1\n",
      "total error =  313 percentError =  52.166666666666664\n",
      "600 test cases: 300 label 0; 300 label 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gpang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: divide by zero encountered in log\n",
      "c:\\users\\gpang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "# Training set (4,800 samples) accuracy is 0.9996, loss is 0.0004\n",
    "# Validation set (480 samples) accuracy is 0.4783, loss is 3.485\n",
    "\n",
    "# To get the results of the ANN using test dataset\n",
    "y_pred = ANN_model.predict(test_x) # 600 cases\n",
    "\n",
    "epsilon = 1e-15  # Small value to prevent log(0)\n",
    "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "print(test_x.shape) # (600, 16384)\n",
    "print(test_y.shape) # (600,)\n",
    "print(y_pred.shape)\n",
    "\n",
    "total_loss = binary_cross_entropy_check(test_y, y_pred)\n",
    "print(f\"** CHECK Average BCE Loss for multiple samples: {total_loss}\") # NaN \n",
    "\n",
    "# To check on accuracy\n",
    "# first, convert the elements in y_pred so that negative becones epsilon, largest is 1 - epsilon\n",
    "epsilon = 1e-15  # Small value to prevent log(0)\n",
    "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "N=len(test_y) # 600 \n",
    "threshold = 0.5 # for calculating accuracy\n",
    "y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(test_y,y_pred_binary)\n",
    "print(\"Accuracy Score:\",accuracy)\n",
    "\n",
    "########################################################\n",
    "#  Count error cases (based on the 600 cases in the test set)\n",
    "########################################################\n",
    "test_y_binary = test_y\n",
    "error0to1 = 0  # test_y ground truth is 0; but predicted as 1\n",
    "error1to0 = 0  # test_y is 1; but predicted as 0\n",
    "for i in range(N):\n",
    "    if (test_y_binary[i] == 0 and y_pred_binary[i] == 1):\n",
    "        error0to1 += 1\n",
    "    if (test_y_binary[i] == 1 and y_pred_binary[i] == 0):\n",
    "        error1to0 += 1\n",
    "print('error0to1 = ',error0to1, '; ground truth label is 0')\n",
    "print('error1to0 = ',error1to0, '; ground truth label is 1')\n",
    "print('total error = ',error0to1+error1to0,' (', 100*(error0to1+error1to0)/N, '%)')\n",
    "print('600 test cases: 300 label 0; 300 label 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800, 16384)\n",
      "(4800,)\n",
      "(4800, 1)\n",
      "** CHECK Average BCE Loss for multiple samples: [nan]\n",
      "Accuracy Score: 0.9404166666666667\n",
      "error0to1 =  0 ; label is 0\n",
      "error1to0 =  286 ; label is 1\n",
      "total error =  286 percentError =  5.958333333333333\n",
      "train cases:  4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gpang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: divide by zero encountered in log\n",
      "c:\\users\\gpang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "# To get the results of the ANN using train dataset\n",
    "y_pred = ANN_model.predict(train_x)\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(y_pred.shape)\n",
    "total_loss = binary_cross_entropy_check(train_y, y_pred)\n",
    "print(f\"** CHECK Average BCE Loss for multiple samples: {total_loss}\")\n",
    "\n",
    "##########################\n",
    "# Check accuracy\n",
    "##########################\n",
    "# convert the elements in y_pred so that negative becones epsilon, largest is 1 - epsilon\n",
    "epsilon = 1e-15  # Small value to prevent log(0)\n",
    "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "N = len(train_y)\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "accuracy = accuracy_score(train_y,y_pred_binary)\n",
    "print(\"Accuracy Score:\",accuracy)\n",
    "\n",
    "##########################\n",
    "# Count error cases:\n",
    "##########################\n",
    "train_y_binary = train_y\n",
    "error0to1 = 0  # test_y is 0\n",
    "error1to0 = 0  # test_y is 1\n",
    "for i in range(N):\n",
    "    if (train_y_binary[i] == 0 and y_pred_binary[i] == 1):\n",
    "        error0to1 += 1\n",
    "    if (train_y_binary[i] == 1 and y_pred_binary[i] == 0):\n",
    "        error1to0 += 1\n",
    "print('error0to1 = ',error0to1, '; label is 0')\n",
    "print('error1to0 = ',error1to0, '; label is 1')\n",
    "print('total error = ',error0to1+error1to0,' (', 100*(error0to1+error1to0)/N, '%)')\n",
    "print('train cases: ', len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01398\n"
     ]
    }
   ],
   "source": [
    "print(np.max(train_x)) #0.01398 (0.01 * 1.4) largest value \n",
    "print(np.min(train_x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO USE BELOW\n",
    "\n",
    "#Define ANOTHER ANN model\n",
    "input_dim = 16384\n",
    "ANNmodel2 = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=(1500,input_dim)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid') # Output layer for binary classification\n",
    "])\n",
    "# compile with cross entropy\n",
    "ANNmodel2.compile(optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "start_time = time.time()\n",
    "history = ANNmodel2.fit(x_train, y_train, epochs=30, batch_size=32,verbose = 2)\n",
    "end_time = time.time()\n",
    "exe_time = end_time - start_time\n",
    "print(\"Execution time: \", exe_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ANNmodel2.evaluate(test_x,test_y,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Binary Cross-Entropy Loss (function): 0.20273661557656092\n",
      "--Average BCE Loss for multiple samples: 0.20273661557656092\n"
     ]
    }
   ],
   "source": [
    "# test code\n",
    "import numpy as np\n",
    "from keras.losses import binary_crossentropy\n",
    "import keras.backend as K\n",
    "\n",
    "# Example true labels and predicted probabilities\n",
    "y_true = np.array([0, 1, 1, 0, 1])\n",
    "y_pred = np.array([0.1, 0.9, 0.8, 0.2, 0.7])\n",
    "\n",
    "# Compute Binary Cross-Entropy using NumPy\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    bce = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return bce\n",
    "\n",
    "bce_loss = binary_cross_entropy(y_true, y_pred)\n",
    "print(f\"***Binary Cross-Entropy Loss (function): {bce_loss}\")\n",
    "\n",
    "#===========================================================================\n",
    "def binary_cross_entropy_np(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates Binary Cross-Entropy loss for multiple samples using NumPy.\n",
    "    y_true: NumPy array of actual labels (0s and 1s)\n",
    "    y_pred: NumPy array of predicted probabilities (between 0 and 1)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # Small value to prevent log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "total_loss = binary_cross_entropy_np(y_true, y_pred)\n",
    "print(f\"--Average BCE Loss for multiple samples: {total_loss}\")\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "# Compute Binary Cross-Entropy using Keras\n",
    "# DOES NOT WORK\n",
    "# bce_loss_keras = binary_crossentropy(K.constant(y_true), K.constant(y_pred)).numpy()\n",
    "# print(f\"Binary Cross-Entropy Loss (Keras): {bce_loss_keras}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4\n",
      "-2.5\n"
     ]
    }
   ],
   "source": [
    "aaa = np.array([0, 1, 1, -2.5, 1 , 3.4])\n",
    "bbb = np.array([0.1, 0.9, 0.8, 0.2, 0.7])\n",
    "\n",
    "print(max(aaa))\n",
    "print(min(aaa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
