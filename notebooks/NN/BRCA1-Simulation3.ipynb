{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN Training\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "# Compute Binary Cross-Entropy using NumPy\n",
    "def binary_cross_entropy_np(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates Binary Cross-Entropy loss for multiple samples using NumPy.\n",
    "    y_true: NumPy array of actual labels (0s and 1s)\n",
    "    y_pred: NumPy array of predicted probabilities (between 0 and 1)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # Small value to prevent log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "# Compute Binary Cross-Entropy using own calculaton for checking\n",
    "def binary_cross_entropy_check(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates Binary Cross-Entropy loss for multiple samples using NumPy.\n",
    "    y_true: NumPy array of actual labels (0s and 1s)\n",
    "    y_pred: NumPy array of predicted probabilities (between 0 and 1)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # Small value to prevent log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "    \n",
    "    N=len(y_true)\n",
    "    BCE_sum=0\n",
    "    for i in range(N):\n",
    "        BCE_sum = BCE_sum + y_true[i]*np.log(y_pred[i]) + (1-y_true[i])*np.log((1-y_pred[i])) \n",
    "\n",
    "    BCE = (-1*BCE_sum)/N\n",
    "    return BCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             (None, 256)               512256    \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 531,105\n",
      "Trainable params: 530,433\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# To define some ANN models\n",
    "\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_dim = 2000))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid')) # Output layer for binary classification\n",
    "    \n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def baseline_model2():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu', input_dim = 2000))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid')) # Output layer for binary classification\n",
    "    \n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def baseline_model3():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_dim = 2000))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid')) # Output layer for binary classification\n",
    "    \n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Build the model\n",
    "# ANN_model = baseline_model()\n",
    "\n",
    "ANN_model = baseline_model2()\n",
    "\n",
    "ANN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the balanced training and testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the dataset\n",
    "# Ref vector is 1x1000; same for variant vector\n",
    "def generate_dataset(num_items,size_ref_vector,label_1_matrix,label_0_matrix,label_1_sign,label_0_sign):\n",
    "    train_x = np.empty((0,2*size_ref_vector))\n",
    "    train_y = np.empty((0,1))\n",
    "    lowerB = 0.99\n",
    "    upperB = 1.01\n",
    "    lowerR = 1.2  # modify individual element of variant vector\n",
    "    upperR = 1.5  # in range [1.2, 1.5]\n",
    "\n",
    "    for i in range(0,num_items):\n",
    "        #Ref vector\n",
    "        ref_vector1 = np.array(np.random.uniform(low=0.01, high=0.6, size=size_ref_vector))\n",
    "        # Initial variant is obtained from deepcopy\n",
    "        k_adjust = random.uniform(lowerB,upperB)\n",
    "\n",
    "        var_vector1 = copy.deepcopy(ref_vector1)*k_adjust\n",
    "\n",
    "        index = (i % 80)  #label 1 40 cases, label 0 also 40 cases\n",
    "        \n",
    "        # Generate 40 lABEL 1 data points\n",
    "        for icase in range(0,40):\n",
    "            pos = np.array(label_1_matrix[icase,:])\n",
    "            kk_sign = np.array(label_1_sign[icase,:])\n",
    "            counter = 0\n",
    "            for j in pos:\n",
    "                kk = random.uniform(lowerR,upperR)   \n",
    "                var_vector1[j] = ref_vector1[j]*kk*kk_sign[counter]\n",
    "                # debug print('--- ', icase,j,kk,counter,kk_sign[counter])\n",
    "                counter = counter + 1\n",
    "\n",
    "            zzz_x = np.concatenate((ref_vector1,var_vector1))   \n",
    "            zzz_y = np.array([1])\n",
    "            train_x = np.append(train_x,[zzz_x],axis=0)\n",
    "            train_y = np.append(train_y,[zzz_y],axis=0)\n",
    "\n",
    "        # Generate 40 lABEL 0 data points\n",
    "        for icase in range(0,40):\n",
    "            pos = np.array(label_0_matrix[icase,:])\n",
    "            kk_sign = np.array(label_0_sign[icase,:])\n",
    "            counter = 0\n",
    "            for j in pos:\n",
    "                kk = random.uniform(lowerR,upperR)\n",
    "                var_vector1[j] = ref_vector1[j]*kk*kk_sign[counter]\n",
    "\n",
    "            zzz_x = np.concatenate((ref_vector1,var_vector1))   \n",
    "            zzz_y = np.array([0])\n",
    "            train_x = np.append(train_x,[zzz_x],axis=0)\n",
    "            train_y = np.append(train_y,[zzz_y],axis=0)\n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_1_matrix.shape:  (40, 30)\n",
      "label_0_matrix.shape:  (40, 30)\n",
      "label_1_sign.shape:  (40, 30)\n",
      "label_0_sign.shape:  (40, 30)\n",
      "Generating the training dataset ...\n",
      "(4000, 2000)\n",
      "(4000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_elements_to_change = 30\n",
    "size_ref_vector = 1000\n",
    "label_1_matrix = np.random.randint(0,size_ref_vector-1, size=(40,num_elements_to_change) )\n",
    "label_0_matrix = np.random.randint(0,size_ref_vector-1, size=(40,num_elements_to_change) )\n",
    "\n",
    "label_1_sign = np.random.choice([-1,1], size=(40,num_elements_to_change) )\n",
    "label_0_sign = np.random.choice([-1,1], size=(40,num_elements_to_change) )\n",
    "\n",
    "print('label_1_matrix.shape: ' , label_1_matrix.shape)\n",
    "print('label_0_matrix.shape: ' , label_0_matrix.shape)\n",
    "print('label_1_sign.shape: ' , label_1_sign.shape)\n",
    "print('label_0_sign.shape: ' , label_0_sign.shape)\n",
    "\n",
    "Nset = 50\n",
    "# Each set of dataset is Nset x 80\n",
    "print('Generating the training dataset ...')\n",
    "train_x, train_y=generate_dataset(Nset,size_ref_vector,label_1_matrix,label_0_matrix,label_1_sign,label_0_sign)\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the testing dataset ...\n",
      "(2000, 2000)\n",
      "(2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Each set of dataset is Nset x 80\n",
    "print('Generating the testing dataset ...')\n",
    "test_x, test_y=generate_dataset(25,size_ref_vector,label_1_matrix,label_0_matrix,label_1_sign,label_0_sign)\n",
    "print(test_x.shape) # (2000, 2000)\n",
    "print(test_y.shape) # (2000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "54/54 - 1s - loss: 0.1016 - accuracy: 0.9679 - val_loss: 0.0166 - val_accuracy: 0.9917\n",
      "Epoch 2/50\n",
      "54/54 - 0s - loss: 0.0376 - accuracy: 0.9906 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
      "Epoch 3/50\n",
      "54/54 - 0s - loss: 0.0232 - accuracy: 0.9950 - val_loss: 0.0087 - val_accuracy: 0.9950\n",
      "Epoch 4/50\n",
      "54/54 - 0s - loss: 0.0154 - accuracy: 0.9968 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "54/54 - 0s - loss: 0.0097 - accuracy: 0.9988 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "54/54 - 0s - loss: 0.0094 - accuracy: 0.9976 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "54/54 - 0s - loss: 0.0125 - accuracy: 0.9974 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "54/54 - 0s - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "54/54 - 0s - loss: 0.0084 - accuracy: 0.9979 - val_loss: 5.6561e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "54/54 - 0s - loss: 0.0047 - accuracy: 0.9994 - val_loss: 5.0430e-04 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "54/54 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 5.1743e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "54/54 - 0s - loss: 0.0045 - accuracy: 0.9985 - val_loss: 6.9797e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "54/54 - 0s - loss: 0.0100 - accuracy: 0.9965 - val_loss: 3.9619e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "54/54 - 0s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.5883e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "54/54 - 0s - loss: 0.0157 - accuracy: 0.9974 - val_loss: 4.1987e-04 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "54/54 - 0s - loss: 0.0174 - accuracy: 0.9935 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "54/54 - 0s - loss: 0.0063 - accuracy: 0.9988 - val_loss: 4.8366e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "54/54 - 0s - loss: 0.0062 - accuracy: 0.9982 - val_loss: 4.6463e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "54/54 - 0s - loss: 0.0135 - accuracy: 0.9959 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "54/54 - 0s - loss: 0.0093 - accuracy: 0.9979 - val_loss: 4.0074e-04 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "54/54 - 0s - loss: 0.0066 - accuracy: 0.9976 - val_loss: 3.3409e-04 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "54/54 - 0s - loss: 0.0046 - accuracy: 0.9988 - val_loss: 4.1232e-04 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "54/54 - 0s - loss: 0.0055 - accuracy: 0.9982 - val_loss: 3.8471e-04 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "54/54 - 0s - loss: 0.0131 - accuracy: 0.9974 - val_loss: 0.0116 - val_accuracy: 0.9967\n",
      "Epoch 25/50\n",
      "54/54 - 0s - loss: 0.0087 - accuracy: 0.9976 - val_loss: 0.0046 - val_accuracy: 0.9983\n",
      "Epoch 26/50\n",
      "54/54 - 0s - loss: 0.0030 - accuracy: 0.9997 - val_loss: 0.0059 - val_accuracy: 0.9983\n",
      "Epoch 27/50\n",
      "54/54 - 0s - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0083 - val_accuracy: 0.9983\n",
      "Epoch 28/50\n",
      "54/54 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0093 - val_accuracy: 0.9983\n",
      "Epoch 29/50\n",
      "54/54 - 0s - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.0066 - val_accuracy: 0.9983\n",
      "Epoch 30/50\n",
      "54/54 - 0s - loss: 0.0110 - accuracy: 0.9971 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "54/54 - 0s - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0027 - val_accuracy: 0.9983\n",
      "Epoch 32/50\n",
      "54/54 - 0s - loss: 0.0045 - accuracy: 0.9985 - val_loss: 0.0040 - val_accuracy: 0.9983\n",
      "Epoch 33/50\n",
      "54/54 - 0s - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0064 - val_accuracy: 0.9983\n",
      "Epoch 34/50\n",
      "54/54 - 0s - loss: 0.0041 - accuracy: 0.9994 - val_loss: 0.0040 - val_accuracy: 0.9983\n",
      "Epoch 35/50\n",
      "54/54 - 0s - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0027 - val_accuracy: 0.9983\n",
      "Epoch 36/50\n",
      "54/54 - 0s - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.0052 - val_accuracy: 0.9983\n",
      "Epoch 37/50\n",
      "54/54 - 0s - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.0068 - val_accuracy: 0.9983\n",
      "Epoch 38/50\n",
      "54/54 - 0s - loss: 0.0035 - accuracy: 0.9994 - val_loss: 7.5730e-04 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "54/54 - 0s - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0052 - val_accuracy: 0.9983\n",
      "Epoch 40/50\n",
      "54/54 - 0s - loss: 0.0026 - accuracy: 0.9997 - val_loss: 8.0831e-04 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "54/54 - 0s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 0.9983\n",
      "Epoch 42/50\n",
      "54/54 - 0s - loss: 0.0043 - accuracy: 0.9991 - val_loss: 0.0038 - val_accuracy: 0.9983\n",
      "Epoch 43/50\n",
      "54/54 - 0s - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0030 - val_accuracy: 0.9983\n",
      "Epoch 44/50\n",
      "54/54 - 0s - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0040 - val_accuracy: 0.9983\n",
      "Epoch 45/50\n",
      "54/54 - 0s - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "54/54 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 0.9983\n",
      "Epoch 47/50\n",
      "54/54 - 0s - loss: 9.0544e-04 - accuracy: 1.0000 - val_loss: 6.4113e-04 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "54/54 - 0s - loss: 6.5659e-04 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 0.9983\n",
      "Epoch 49/50\n",
      "54/54 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 0.9983\n",
      "Epoch 50/50\n",
      "54/54 - 0s - loss: 8.4680e-04 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 0.9983\n",
      "Execution time:  11.480006694793701\n",
      "63/63 - 0s - loss: 1.2984e-04 - accuracy: 1.0000\n",
      "Testing Accuracy =  [0.00012984091881662607, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "start_time = time.time()\n",
    "history = ANN_model.fit(train_x, train_y, epochs=50, batch_size=64,validation_split=0.15,verbose = 2)\n",
    "end_time = time.time()\n",
    "exe_time = end_time - start_time\n",
    "print(\"Execution time: \", exe_time)\n",
    "scores = ANN_model.evaluate(test_x,test_y,verbose = 2)\n",
    "print(\"Testing Accuracy = \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  max and min \n",
      "0.9999943\n",
      "2.025336e-05\n",
      "(2000, 2000)\n",
      "(2000, 1)\n",
      "(2000, 1)\n",
      "** CHECK Average BCE Loss for multiple samples: [0.0001]\n",
      "Accuracy Score: 1.0\n",
      "error0to1 =  0 ; label is 0\n",
      "error1to0 =  0 ; label is 1\n",
      "Testing total error =  0 percentError =  0.0\n",
      "2000 test cases: 1000 label 0; 1000 label 1\n"
     ]
    }
   ],
   "source": [
    "# To get the results of the ANN using test dataset\n",
    "y_pred = ANN_model.predict(test_x)\n",
    "\n",
    "print('  max and min ')\n",
    "print(np.max(y_pred))\n",
    "print(np.min(y_pred))\n",
    "\n",
    "epsilon = 1e-15  # Small value to prevent log(0)\n",
    "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "print(y_pred.shape)\n",
    "\n",
    "total_loss = binary_cross_entropy_check(test_y, y_pred)\n",
    "print(f\"** Average BCE Loss for multiple samples: {total_loss}\")\n",
    "\n",
    "# To check on accuracy\n",
    "# first, convert the elements in y_pred so that negative becones epsilon, largest is 1 - epsilon\n",
    "epsilon = 1e-15  # Small value to prevent log(0)\n",
    "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "N=len(test_y)\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(test_y,y_pred_binary)\n",
    "print(\"Accuracy Score:\",accuracy)\n",
    "\n",
    "#Count error cases:\n",
    "#\n",
    "test_y_binary = test_y\n",
    "error0to1 = 0  # test_y is 0\n",
    "error1to0 = 0  # test_y is 1\n",
    "for i in range(N):\n",
    "    if (test_y_binary[i] == 0 and y_pred_binary[i] == 1):\n",
    "        error0to1 += 1\n",
    "    if (test_y_binary[i] == 1 and y_pred_binary[i] == 0):\n",
    "        error1to0 += 1\n",
    "        \n",
    "print('error0to1 = ',error0to1, '; label is 0')\n",
    "print('error1to0 = ',error1to0, '; label is 1')\n",
    "print('Testing total error = ',error0to1+error1to0,'percentError = ', 100*(error0to1+error1to0)/N)\n",
    "print('2000 test cases: 1000 label 0; 1000 label 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_x.shape  (2000, 2000)\n",
      "test_y.shape  (2000, 1)\n",
      "(2000, 1)\n",
      "Compare between test_y and y_pred_binary\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sum of errors =  0\n"
     ]
    }
   ],
   "source": [
    "print('test_x.shape ', test_x.shape)\n",
    "print('test_y.shape ',test_y.shape)\n",
    "print(y_pred_binary.shape)\n",
    "\n",
    "print('Compare between test_y and y_pred_binary')\n",
    "\n",
    "errorcase=[0]*80\n",
    "\n",
    "for i in range(0,len(test_y)):\n",
    "    index = (i % 80)\n",
    "    if (test_y[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "print(errorcase)\n",
    "print('Sum of errors = ' ,np.sum(errorcase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 2000)\n",
      "(4000, 1)\n",
      "(4000, 1)\n",
      "** CHECK Average BCE Loss for multiple samples: [0.0005]\n",
      "Accuracy Score: 0.99975\n",
      "error0to1 =  1 ; label is 0\n",
      "error1to0 =  0 ; label is 1\n",
      "Training total error =  1 percentError =  0.025\n",
      "train cases:  4000\n"
     ]
    }
   ],
   "source": [
    "# To get the results of the ANN using train dataset\n",
    "y_pred = ANN_model.predict(train_x)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(y_pred.shape)\n",
    "\n",
    "total_loss = binary_cross_entropy_check(train_y, y_pred)\n",
    "print(f\"** CHECK Average BCE Loss for multiple samples: {total_loss}\")\n",
    "\n",
    "# To check on accuracy\n",
    "# first, convert the elements in y_pred so that negative becones epsilon, largest is 1 - epsilon\n",
    "epsilon = 1e-15  # Small value to prevent log(0)\n",
    "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "\n",
    "N = len(train_y)\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(train_y,y_pred_binary)\n",
    "print(\"Accuracy Score:\",accuracy)\n",
    "\n",
    "#Count error cases:\n",
    "#\n",
    "train_y_binary = train_y\n",
    "error0to1 = 0  # test_y is 0\n",
    "error1to0 = 0  # test_y is 1\n",
    "for i in range(N):\n",
    "    if (train_y_binary[i] == 0 and y_pred_binary[i] == 1):\n",
    "        error0to1 += 1\n",
    "    if (train_y_binary[i] == 1 and y_pred_binary[i] == 0):\n",
    "        error1to0 += 1\n",
    "        \n",
    "print('error0to1 = ',error0to1, '; label is 0')\n",
    "print('error1to0 = ',error1to0, '; label is 1')\n",
    "print('Training total error = ',error0to1+error1to0,'percentError = ', 100*(error0to1+error1to0)/N)\n",
    "print('train cases: ', len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1 323 177 ... 543 482 526]\n",
      " [702 197 859 ... 363 193 356]\n",
      " [313 253 407 ... 323 510 157]\n",
      " ...\n",
      " [737  69 738 ... 918  32 893]\n",
      " [647 697 271 ... 621   1  17]\n",
      " [853 299 323 ... 428 438 636]]\n"
     ]
    }
   ],
   "source": [
    "print(label_1_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3039 0.3006 0.3049 ... 0.3006 0.3049 0.4289]\n",
      " [0.3045 0.3033 0.3086 ... 0.3599 0.3086 0.3018]\n",
      " [0.3049 0.3029 0.3019 ... 0.3978 0.3019 0.4091]\n",
      " ...\n",
      " [0.3108 0.3029 0.2993 ... 0.3029 0.2993 0.3026]\n",
      " [0.3005 0.3072 0.3018 ... 0.3072 0.4452 0.3074]\n",
      " [0.3052 0.3038 0.3063 ... 0.3038 0.3062 0.2989]]\n"
     ]
    }
   ],
   "source": [
    "# DELETE below\n",
    "# Part 2 of investigation\n",
    "# \n",
    "#\n",
    "DBset = np.empty((0,8))\n",
    "\n",
    "for i in range(0,2000):\n",
    "    #Ref vector\n",
    "    aa=np.mean(train_x[i,0:2000])\n",
    "    bb=np.mean(train_x[i,2000:4000])\n",
    "    cc=np.mean(train_x[i,4000:6000])\n",
    "    dd=np.mean(train_x[i,6000:8000])\n",
    "    #Variant vector\n",
    "    ee = np.mean(train_x[i,8192:10192])\n",
    "    ff = np.mean(train_x[i,10192:12192])\n",
    "    gg = np.mean(train_x[i,12192:14192])\n",
    "    hh = np.mean(train_x[i,14192:16192])\n",
    "    \n",
    "    index = (i % 8)\n",
    "    # index 0,1,2,3 are labe 1; index 4,5,6,7 are labe 0; \n",
    "    lowerR = 1.1\n",
    "    upperR = 1.45\n",
    "    \n",
    "    if (index == 0): \n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee, ff, gg, hh*kk])\n",
    "    if (index == 1):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee, ff*kk, gg, hh])\n",
    "    if (index == 2):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee, ff*kk, gg, hh*kk])\n",
    "    if (index == 3):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee*kk, ff*kk, gg*kk, hh*kk])   \n",
    "        \n",
    "    if (index == 4):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee, ff, gg*kk, hh])\n",
    "    if (index == 5):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee*kk, ff, gg, hh])\n",
    "    if (index == 6):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee*kk, ff, gg*kk, hh])\n",
    "    if (index == 7):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee, ff, gg, hh])  \n",
    "    \n",
    "    DBset = np.append(DBset,[zzz],axis=0)\n",
    "    # np.set_printoptions(precision=4)\n",
    "    # print(zzz)\n",
    "    \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test set of 2000\n",
    "\n",
    "DBset_test = np.empty((0,8))\n",
    "\n",
    "for i in range(0,2000):\n",
    "    #Ref vector\n",
    "    aa=np.mean(test_x[i,0:2000])\n",
    "    bb=np.mean(test_x[i,2000:4000])\n",
    "    cc=np.mean(test_x[i,4000:6000])\n",
    "    dd=np.mean(test_x[i,6000:8000])\n",
    "    #Variant vector\n",
    "    ee = np.mean(test_x[i,8192:10192])\n",
    "    ff = np.mean(test_x[i,10192:12192])\n",
    "    gg = np.mean(test_x[i,12192:14192])\n",
    "    hh = np.mean(test_x[i,14192:16192])\n",
    "    \n",
    "    index = (i % 8)\n",
    "    # index 0,1,2,3 are labe 1; index 4,5,6,7 are labe 0; \n",
    "    lowerR = 1.1\n",
    "    upperR = 1.45\n",
    "    \n",
    "    if (index == 0): \n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee, ff, gg, hh*kk])\n",
    "    if (index == 1):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee, ff*kk, gg, hh])\n",
    "    if (index == 2):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee, ff*kk, gg, hh*kk])\n",
    "    if (index == 3):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee*kk, ff*kk, gg*kk, hh*kk])   \n",
    "        \n",
    "    if (index == 4):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee, ff, gg*kk, hh])\n",
    "    if (index == 5):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee*kk, ff, gg, hh])\n",
    "    if (index == 6):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee*kk, ff, gg*kk, hh])\n",
    "    if (index == 7):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee, ff, gg, hh])  \n",
    "    \n",
    "    DBset_test = np.append(DBset_test,[zzz],axis=0)\n",
    "    # np.set_printoptions(precision=4)\n",
    "    # print(zzz)\n",
    "    \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run\n",
    "# debug\n",
    "#\n",
    "DBset_test = np.empty((0,8))\n",
    "\n",
    "for i in range(0,2000):\n",
    "    #Ref vector\n",
    "    aa=np.mean(test_x[i,0:2000])\n",
    "    bb=np.mean(test_x[i,2000:4000])\n",
    "    cc=np.mean(test_x[i,4000:6000])\n",
    "    dd=np.mean(test_x[i,6000:8000])\n",
    "    #Variant vector\n",
    "    ee = np.mean(test_x[i,8192:10192])\n",
    "    ff = np.mean(test_x[i,10192:12192])\n",
    "    gg = np.mean(test_x[i,12192:14192])\n",
    "    hh = np.mean(test_x[i,14192:16192])\n",
    "    \n",
    "    index = (i % 8)\n",
    "    # index 0,1,2,3 are labe 1; index 4,5,6,7 are labe 0; \n",
    "    lowerR = 1.1\n",
    "    upperR = 1.45\n",
    "    \n",
    "    if (index == 0): \n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee, ff, gg, hh*kk])\n",
    "    if (index == 1):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee, ff*kk, gg, hh])\n",
    "    if (index == 2):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee, ff*kk, gg, hh*kk])\n",
    "    if (index == 3):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee*kk, ff*kk, gg*kk, hh*kk])   \n",
    "        \n",
    "    if (index == 4):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee, ff, gg*kk, hh])\n",
    "    if (index == 5):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee*kk, ff, gg, hh])\n",
    "    if (index == 6):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee*kk, ff, gg*kk, hh])\n",
    "    if (index == 7):\n",
    "        kk = random.uniform(lowerR,upperR)\n",
    "        zzz = np.array([aa, bb, cc, dd, ee, ff, gg, hh])  \n",
    "    \n",
    "    DBset_test = np.append(DBset_test,[zzz],axis=0)\n",
    "    # np.set_printoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 8)\n",
      "(2000,)\n",
      "[1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "(2000, 8)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "    print(DBset.shape)\n",
    "    DBy = train_y[0:2000]\n",
    "    print(DBy.shape)\n",
    "    \n",
    "    print(DBy[0:16])\n",
    "    \n",
    "    print(DBset_test.shape)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/gpang/iCloudDrive/Naja/20250628-Evo2/code\\trainX_vectors3.csv\n",
      "Finished Export of Training cases \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "DIR = \"C:/Users/gpang/iCloudDrive/Naja/20250628-Evo2/code\"\n",
    "out_path = os.path.join(DIR, \"trainX_vectors3.csv\")\n",
    "print(out_path)\n",
    "# DB_train.tofile(out_path,sep=',',fmt='%.5f')\n",
    "np.savetxt(out_path,DBset,delimiter=',',fmt='%.5f')\n",
    "print(\"Finished Export of Training cases \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/gpang/iCloudDrive/Naja/20250628-Evo2/code\\trainY_vectors3.csv\n",
      "Finished Export of Testing cases \n"
     ]
    }
   ],
   "source": [
    "out_path = os.path.join(DIR, \"trainY_vectors3.csv\")\n",
    "print(out_path)\n",
    "# DB_test.tofile(out_path,sep=',',fmt='#.5f')\n",
    "np.savetxt(out_path,DBy,delimiter=',',fmt='%.5f')\n",
    "print(\"Finished Export of Testing cases \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/gpang/iCloudDrive/Naja/20250628-Evo2/code\\testX_vectors3.csv\n",
      "Finished Export of Training cases \n"
     ]
    }
   ],
   "source": [
    "out_path = os.path.join(DIR, \"testX_vectors3.csv\")\n",
    "print(out_path)\n",
    "# DB_train.tofile(out_path,sep=',',fmt='%.5f')\n",
    "np.savetxt(out_path,DBset_test,delimiter=',',fmt='%.5f')\n",
    "print(\"Finished Export of Training cases \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 721\n",
      "Trainable params: 641\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def baseline_model3():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_dim = 8))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid')) # Output layer for binary classification\n",
    "    \n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "ANN_model = baseline_model3()\n",
    "\n",
    "ANN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "25/25 - 1s - loss: 0.8202 - accuracy: 0.4313 - val_loss: 0.6919 - val_accuracy: 0.5000\n",
      "Epoch 2/40\n",
      "25/25 - 0s - loss: 0.5825 - accuracy: 0.7013 - val_loss: 0.6846 - val_accuracy: 0.5000\n",
      "Epoch 3/40\n",
      "25/25 - 0s - loss: 0.4539 - accuracy: 0.8669 - val_loss: 0.6780 - val_accuracy: 0.5000\n",
      "Epoch 4/40\n",
      "25/25 - 0s - loss: 0.3591 - accuracy: 0.9306 - val_loss: 0.6705 - val_accuracy: 0.5000\n",
      "Epoch 5/40\n",
      "25/25 - 0s - loss: 0.2948 - accuracy: 0.9444 - val_loss: 0.6604 - val_accuracy: 0.5000\n",
      "Epoch 6/40\n",
      "25/25 - 0s - loss: 0.2442 - accuracy: 0.9494 - val_loss: 0.6488 - val_accuracy: 0.5000\n",
      "Epoch 7/40\n",
      "25/25 - 0s - loss: 0.1802 - accuracy: 0.9731 - val_loss: 0.6314 - val_accuracy: 0.5000\n",
      "Epoch 8/40\n",
      "25/25 - 0s - loss: 0.1544 - accuracy: 0.9825 - val_loss: 0.6261 - val_accuracy: 0.5000\n",
      "Epoch 9/40\n",
      "25/25 - 0s - loss: 0.1425 - accuracy: 0.9750 - val_loss: 0.6233 - val_accuracy: 0.5000\n",
      "Epoch 10/40\n",
      "25/25 - 0s - loss: 0.1214 - accuracy: 0.9825 - val_loss: 0.6233 - val_accuracy: 0.5000\n",
      "Epoch 11/40\n",
      "25/25 - 0s - loss: 0.1039 - accuracy: 0.9869 - val_loss: 0.6390 - val_accuracy: 0.5000\n",
      "Epoch 12/40\n",
      "25/25 - 0s - loss: 0.0989 - accuracy: 0.9800 - val_loss: 0.6719 - val_accuracy: 0.5000\n",
      "Epoch 13/40\n",
      "25/25 - 0s - loss: 0.0854 - accuracy: 0.9862 - val_loss: 0.5649 - val_accuracy: 0.5050\n",
      "Epoch 14/40\n",
      "25/25 - 0s - loss: 0.0793 - accuracy: 0.9862 - val_loss: 0.5185 - val_accuracy: 0.5400\n",
      "Epoch 15/40\n",
      "25/25 - 0s - loss: 0.0654 - accuracy: 0.9931 - val_loss: 0.4571 - val_accuracy: 0.5775\n",
      "Epoch 16/40\n",
      "25/25 - 0s - loss: 0.0658 - accuracy: 0.9906 - val_loss: 0.5082 - val_accuracy: 0.5550\n",
      "Epoch 17/40\n",
      "25/25 - 0s - loss: 0.0528 - accuracy: 0.9931 - val_loss: 0.5214 - val_accuracy: 0.5525\n",
      "Epoch 18/40\n",
      "25/25 - 0s - loss: 0.0559 - accuracy: 0.9906 - val_loss: 0.3317 - val_accuracy: 0.8125\n",
      "Epoch 19/40\n",
      "25/25 - 0s - loss: 0.0498 - accuracy: 0.9919 - val_loss: 0.3458 - val_accuracy: 0.7725\n",
      "Epoch 20/40\n",
      "25/25 - 0s - loss: 0.0454 - accuracy: 0.9950 - val_loss: 0.2367 - val_accuracy: 0.9700\n",
      "Epoch 21/40\n",
      "25/25 - 0s - loss: 0.0406 - accuracy: 0.9931 - val_loss: 0.1497 - val_accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "25/25 - 0s - loss: 0.0374 - accuracy: 0.9956 - val_loss: 0.1087 - val_accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "25/25 - 0s - loss: 0.0369 - accuracy: 0.9950 - val_loss: 0.0536 - val_accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "25/25 - 0s - loss: 0.0330 - accuracy: 0.9950 - val_loss: 0.0418 - val_accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "25/25 - 0s - loss: 0.0293 - accuracy: 0.9962 - val_loss: 0.0507 - val_accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "25/25 - 0s - loss: 0.0302 - accuracy: 0.9950 - val_loss: 0.1521 - val_accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "25/25 - 0s - loss: 0.0243 - accuracy: 0.9975 - val_loss: 0.1224 - val_accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "25/25 - 0s - loss: 0.0235 - accuracy: 0.9987 - val_loss: 0.0945 - val_accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "25/25 - 0s - loss: 0.0209 - accuracy: 0.9975 - val_loss: 0.0692 - val_accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "25/25 - 0s - loss: 0.0208 - accuracy: 0.9975 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "25/25 - 0s - loss: 0.0192 - accuracy: 0.9975 - val_loss: 0.3549 - val_accuracy: 0.7750\n",
      "Epoch 32/40\n",
      "25/25 - 0s - loss: 0.0154 - accuracy: 0.9981 - val_loss: 0.0185 - val_accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "25/25 - 0s - loss: 0.0136 - accuracy: 0.9994 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "25/25 - 0s - loss: 0.0147 - accuracy: 0.9987 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "25/25 - 0s - loss: 0.0117 - accuracy: 0.9994 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "25/25 - 0s - loss: 0.0172 - accuracy: 0.9975 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "25/25 - 0s - loss: 0.0139 - accuracy: 0.9969 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "25/25 - 0s - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "25/25 - 0s - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "25/25 - 0s - loss: 0.0160 - accuracy: 0.9962 - val_loss: 0.2511 - val_accuracy: 0.8500\n",
      "Execution time:  2.315142869949341\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "history = ANN_model.fit(DBset, DBy, epochs=40, batch_size=64,validation_split=0.2,verbose = 2)\n",
    "\n",
    "end_time = time.time()\n",
    "exe_time = end_time - start_time\n",
    "print(\"Execution time: \", exe_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref vector  : ---   ---   ---   --- \n",
      "Var index 0 : ---   ---   ---   -C-   label 1\n",
      "Var index 1 : ---   -C-   ---   ---   label 1\n",
      "Var index 2 : ---   -C-   ---   -C-   label 1\n",
      "Var index 3 : -C-   -C-   -C-   -C-   70% label 1\n",
      "Var index 4 : ---   ---   -C-   ---   label 0\n",
      "Var index 5 : -C-   ---   ---   ---   label 0\n",
      "Var index 6 : -C-   ---   -C-   ---   label 0\n",
      "Var index 7 : -C-   -C-   -C-   -C-   0.3% label 0\n",
      "(2000, 8)\n",
      "(2000,)\n",
      "[1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "[[0.2998 0.3119 0.3061 0.3032 0.2998 0.3119 0.3061 0.4311]\n",
      " [0.3094 0.3061 0.3012 0.3067 0.3094 0.374  0.3012 0.3067]\n",
      " [0.3065 0.3079 0.3024 0.3035 0.3065 0.4077 0.3024 0.4018]\n",
      " [0.3021 0.3061 0.3095 0.3106 0.3375 0.3403 0.3473 0.3523]\n",
      " [0.3054 0.3114 0.3052 0.2988 0.3054 0.3114 0.3381 0.2988]\n",
      " [0.3064 0.3077 0.3141 0.3042 0.3998 0.3077 0.3141 0.3042]\n",
      " [0.3043 0.304  0.3102 0.3022 0.4186 0.304  0.42   0.3022]\n",
      " [0.3065 0.3089 0.3051 0.3037 0.3065 0.3091 0.3052 0.3035]]\n"
     ]
    }
   ],
   "source": [
    "print('Ref vector  : ---   ---   ---   --- ')\n",
    "print('Var index 0 : ---   ---   ---   -C- ', ' label 1')\n",
    "print('Var index 1 : ---   -C-   ---   --- ', ' label 1')\n",
    "print('Var index 2 : ---   -C-   ---   -C- ', ' label 1')\n",
    "print('Var index 3 : -C-   -C-   -C-   -C- ', ' 70% label 1')\n",
    "\n",
    "print('Var index 4 : ---   ---   -C-   --- ', ' label 0')\n",
    "print('Var index 5 : -C-   ---   ---   --- ', ' label 0')\n",
    "print('Var index 6 : -C-   ---   -C-   --- ', ' label 0')\n",
    "print('Var index 7 : -C-   -C-   -C-   -C- ', ' 0.3% label 0')\n",
    "\n",
    "print(DBset_test.shape)\n",
    "print(DBy.shape)\n",
    "print(DBy[0:16])\n",
    "print(DBset_test[0:8,0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  max and min \n",
      "0.9997665\n",
      "0.092390984\n",
      "(2000, 8)\n",
      "(2000,)\n",
      "(2000, 1)\n",
      "** CHECK Average BCE Loss for multiple samples: [0.2466]\n",
      "Accuracy Score: 0.865\n",
      "error0to1 =  270 ; label is 0\n",
      "error1to0 =  0 ; label is 1\n",
      "total error =  270 percentError =  13.5\n",
      "2000 test cases: 1000 label 0; 1000 label 1\n"
     ]
    }
   ],
   "source": [
    "# To get the results of the ANN using test dataset\n",
    "y_pred = ANN_model.predict(DBset_test)\n",
    "\n",
    "print('  max and min ')\n",
    "print(np.max(y_pred))\n",
    "print(np.min(y_pred))\n",
    "\n",
    "epsilon = 1e-15  # Small value to prevent log(0)\n",
    "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "\n",
    "print(DBset_test.shape)\n",
    "print(DBy.shape)\n",
    "print(y_pred.shape)\n",
    "\n",
    "total_loss = binary_cross_entropy_check(DBy, y_pred)\n",
    "print(f\"** CHECK Average BCE Loss for multiple samples: {total_loss}\")\n",
    "\n",
    "# To check on accuracy\n",
    "# first, convert the elements in y_pred so that negative becones epsilon, largest is 1 - epsilon\n",
    "epsilon = 1e-15  # Small value to prevent log(0)\n",
    "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "N=len(DBy)\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(DBy,y_pred_binary)\n",
    "print(\"Accuracy Score:\",accuracy)\n",
    "\n",
    "#Count error cases:\n",
    "#\n",
    "test_y_binary = DBy\n",
    "error0to1 = 0  # test_y is 0\n",
    "error1to0 = 0  # test_y is 1\n",
    "for i in range(N):\n",
    "    if (test_y_binary[i] == 0 and y_pred_binary[i] == 1):\n",
    "        error0to1 += 1\n",
    "    if (test_y_binary[i] == 1 and y_pred_binary[i] == 0):\n",
    "        error1to0 += 1\n",
    "        \n",
    "print('error0to1 = ',error0to1, '; label is 0')\n",
    "print('error1to0 = ',error1to0, '; label is 1')\n",
    "print('total error = ',error0to1+error1to0,'percentError = ', 100*(error0to1+error1to0)/N)\n",
    "print('2000 test cases: 1000 label 0; 1000 label 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 8)\n",
      "(2000,)\n",
      "[1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      "Compare between DBy and y_pred_binary\n",
      "[0, 0, 0, 0, 12, 11, 0, 247]\n"
     ]
    }
   ],
   "source": [
    "print(DBset_test.shape)\n",
    "print(DBy.shape)\n",
    "print(DBy[0:16])\n",
    "#print(y_pred_binary.shape)\n",
    "#print(y_pred_binary[0:16])\n",
    "\n",
    "print('Compare between DBy and y_pred_binary')\n",
    "\n",
    "errorcase=[0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "for i in range(0,2000):\n",
    "    index = (i % 8)\n",
    "    if (index == 0 and DBy[i] != y_pred_binary[i] ): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "    if (index == 1 and DBy[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "    if (index == 2 and DBy[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "    if (index == 3 and DBy[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "    if (index == 4 and DBy[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "    if (index == 5 and DBy[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "    if (index == 6 and DBy[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "    if (index == 7 and DBy[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "print(errorcase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check based on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  max and min \n",
      "0.9997596\n",
      "0.09228331\n",
      "(2000, 8)\n",
      "(2000,)\n",
      "(2000, 1)\n",
      "** CHECK Average BCE Loss for multiple samples: [0.248]\n",
      "Accuracy Score: 0.863\n",
      "error0to1 =  274 ; label is 0\n",
      "error1to0 =  0 ; label is 1\n",
      "total error =  274 percentError =  13.7\n",
      "2000 training cases: 1000 label 0; 1000 label 1\n"
     ]
    }
   ],
   "source": [
    "# To get the results of the ANN using train dataset\n",
    "y_pred = ANN_model.predict(DBset)\n",
    "\n",
    "print('  max and min ')\n",
    "print(np.max(y_pred))\n",
    "print(np.min(y_pred))\n",
    "\n",
    "epsilon = 1e-15  # Small value to prevent log(0)\n",
    "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "\n",
    "print(DBset.shape)\n",
    "print(DBy.shape)\n",
    "print(y_pred.shape)\n",
    "\n",
    "total_loss = binary_cross_entropy_check(DBy, y_pred)\n",
    "print(f\"** CHECK Average BCE Loss for multiple samples: {total_loss}\")\n",
    "\n",
    "# To check on accuracy\n",
    "# first, convert the elements in y_pred so that negative becones epsilon, largest is 1 - epsilon\n",
    "epsilon = 1e-15  # Small value to prevent log(0)\n",
    "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "N=len(DBy)\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(DBy,y_pred_binary)\n",
    "print(\"Accuracy Score:\",accuracy)\n",
    "\n",
    "#Count error cases:\n",
    "#\n",
    "test_y_binary = DBy\n",
    "error0to1 = 0  # test_y is 0\n",
    "error1to0 = 0  # test_y is 1\n",
    "for i in range(N):\n",
    "    if (test_y_binary[i] == 0 and y_pred_binary[i] == 1):\n",
    "        error0to1 += 1\n",
    "    if (test_y_binary[i] == 1 and y_pred_binary[i] == 0):\n",
    "        error1to0 += 1\n",
    "        \n",
    "print('error0to1 = ',error0to1, '; label is 0')\n",
    "print('error1to0 = ',error1to0, '; label is 1')\n",
    "print('total error = ',error0to1+error1to0,'percentError = ', 100*(error0to1+error1to0)/N)\n",
    "print('2000 training cases: 1000 label 0; 1000 label 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: Compare between DBy and y_pred_binary\n",
      "[0, 0, 0, 0, 20, 8, 0, 246]\n",
      "274\n"
     ]
    }
   ],
   "source": [
    "print('Training dataset: Compare between DBy and y_pred_binary')\n",
    "\n",
    "errorcase=[0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "for i in range(0,2000):\n",
    "    index = (i % 8)\n",
    "    if (index == 0 and DBy[i] != y_pred_binary[i] ): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "    if (index == 1 and DBy[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "    if (index == 2 and DBy[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "    if (index == 3 and DBy[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "    if (index == 4 and DBy[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "    if (index == 5 and DBy[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "    if (index == 6 and DBy[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "    if (index == 7 and DBy[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "print(errorcase)\n",
    "print(np.sum(errorcase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO USE BELOW\n",
    "\n",
    "#Define ANOTHER ANN model\n",
    "input_dim = 16384\n",
    "ANNmodel2 = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=(1500,input_dim)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid') # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# compile with cross entropy\n",
    "ANNmodel2.compile(optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = ANNmodel2.fit(x_train, y_train, epochs=30, batch_size=32,verbose = 2)\n",
    "\n",
    "end_time = time.time()\n",
    "exe_time = end_time - start_time\n",
    "print(\"Execution time: \", exe_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ANNmodel2.evaluate(test_x,test_y,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Binary Cross-Entropy Loss (function): 0.20273661557656092\n",
      "--Average BCE Loss for multiple samples: 0.20273661557656092\n"
     ]
    }
   ],
   "source": [
    "# test code\n",
    "import numpy as np\n",
    "from keras.losses import binary_crossentropy\n",
    "import keras.backend as K\n",
    "\n",
    "# Example true labels and predicted probabilities\n",
    "y_true = np.array([0, 1, 1, 0, 1])\n",
    "y_pred = np.array([0.1, 0.9, 0.8, 0.2, 0.7])\n",
    "\n",
    "# Compute Binary Cross-Entropy using NumPy\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    bce = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return bce\n",
    "\n",
    "bce_loss = binary_cross_entropy(y_true, y_pred)\n",
    "print(f\"***Binary Cross-Entropy Loss (function): {bce_loss}\")\n",
    "\n",
    "#===========================================================================\n",
    "def binary_cross_entropy_np(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates Binary Cross-Entropy loss for multiple samples using NumPy.\n",
    "    y_true: NumPy array of actual labels (0s and 1s)\n",
    "    y_pred: NumPy array of predicted probabilities (between 0 and 1)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # Small value to prevent log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "total_loss = binary_cross_entropy_np(y_true, y_pred)\n",
    "print(f\"--Average BCE Loss for multiple samples: {total_loss}\")\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "# Compute Binary Cross-Entropy using Keras\n",
    "# DOES NOT WORK\n",
    "# bce_loss_keras = binary_crossentropy(K.constant(y_true), K.constant(y_pred)).numpy()\n",
    "# print(f\"Binary Cross-Entropy Loss (Keras): {bce_loss_keras}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4\n",
      "-2.5\n"
     ]
    }
   ],
   "source": [
    "aaa = np.array([0, 1, 1, -2.5, 1 , 3.4])\n",
    "bbb = np.array([0.1, 0.9, 0.8, 0.2, 0.7])\n",
    "\n",
    "print(max(aaa))\n",
    "print(min(aaa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    DBset = np.empty((0,4))\n",
    "    DBset_tmp = np.array([1,2,3,4])\n",
    "    DBset = np.append(DBset,[DBset_tmp],axis=0)\n",
    "    print(DBset)\n",
    "    \n",
    "    DBset_tmp = np.array([5,6,7,8])\n",
    "    DBset = np.append(DBset,[DBset_tmp],axis=0)\n",
    "\n",
    "    print(DBset)\n",
    "    print('========================')\n",
    "    DBset_tmp = np.array([9,10,11,12])\n",
    "    DBset = np.append(DBset,[DBset_tmp],axis=0)\n",
    "\n",
    "    print(DBset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO LOAD / READIN\n",
    "\n",
    "# READ IN CSV (takes 30 seconds)\n",
    "new_train_values = np.loadtxt(\"train_vectors2.csv\",delimiter=\",\")\n",
    "print(new_train_values.shape)\n",
    "new_train = new_train_values.reshape((4000,16385))\n",
    "print('new_train.shape = ',new_train.shape)\n",
    "\n",
    "\n",
    "new_test_values = np.loadtxt(\"test_vectors2.csv\",delimiter=\",\")\n",
    "print(new_test_values.shape)\n",
    "new_test = new_test_values.reshape((4000,16385))\n",
    "print('new_test.shape = ',new_test.shape)\n",
    "\n",
    "train_x = new_train[:,0:16384] \n",
    "train_y = new_train[:,16384]\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "#===================\n",
    "test_x = new_test[:,0:16384] \n",
    "test_y = new_test[:,16384]\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
