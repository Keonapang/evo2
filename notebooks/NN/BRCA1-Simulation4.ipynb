{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN Training\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "def binary_cross_entropy_np(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates Binary Cross-Entropy loss for multiple samples using NumPy.\n",
    "    y_true: NumPy array of actual labels (0s and 1s)\n",
    "    y_pred: NumPy array of predicted probabilities (between 0 and 1)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # Small value to prevent log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "# Compute Binary Cross-Entropy using own calculaton for checking\n",
    "def binary_cross_entropy_check(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates Binary Cross-Entropy loss for multiple samples using NumPy.\n",
    "    y_true: NumPy array of actual labels (0s and 1s)\n",
    "    y_pred: NumPy array of predicted probabilities (between 0 and 1)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # Small value to prevent log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "    N=len(y_true)\n",
    "    BCE_sum=0\n",
    "    for i in range(N):\n",
    "        BCE_sum = BCE_sum + y_true[i]*np.log(y_pred[i]) + (1-y_true[i])*np.log((1-y_pred[i])) \n",
    "    BCE = (-1*BCE_sum)/N\n",
    "    return BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input dataset\n",
    "\n",
    "`generate_dataset_new2()` is the main function. The input is 1x2000 which is  [ ref vector  variant vector ]\n",
    "\n",
    "The training dataset (balanced) is as follows:\n",
    "*  [one_ref_vector  variant_vector_1 (80 locations modified)]    label 1: 10 cases\n",
    "*  [one_ref_vector  variant_vector_2 (20 locations modified)]    label 0: 5 cases\n",
    "*  [one_ref_vector  variant_vector_3 similar to one_ref_vector]  label 0: 5 cases (similar means multiplied by a TINY magnitude)\n",
    "\n",
    "Benign variants is designed to have changes of 2% of elements (label 0). Pathogenic variants have a higher frequency of alterations of 10% of elements(label 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref vector is 1x1000; variant vector is 1x1000\n",
    "def generate_dataset_new2(num_items,size_ref_vector,label_1_matrix,label_0_matrix,label_1_sign,label_0_sign):\n",
    "    train_x = np.empty((0,2*size_ref_vector))\n",
    "    train_y = np.empty((0,1))\n",
    "    # \"similar\" variant is obtained by multiplying the ref vector by a factor in range [0.99, 1.01]\n",
    "    lowerB = 0.99\n",
    "    upperB = 1.01\n",
    "    # Change elements (those 80 locations) of the variant vector by a factor in range [1.2, 1.5]\n",
    "    lowerR = 1.2  # modify individual element of variant vector\n",
    "    upperR = 1.5 \n",
    "\n",
    "    for i in range(0,num_items):\n",
    "        \n",
    "        ref_vector1 = np.array(np.random.uniform(low=0.01, high=0.6, size=size_ref_vector))\n",
    "        # NO USE: index = (i % 20)  #label 1 10 cases, label 0 also 10 cases\n",
    "        # lABEL 1\n",
    "        for icase in range(0,10):\n",
    "            pos = np.array(label_1_matrix[icase,:]) # 1 x 80 \n",
    "            kk_sign = np.array(label_1_sign[icase,:])\n",
    "            counter = 0\n",
    "            \n",
    "            # Initial variant vector (from copying ref vector)* adjustment\n",
    "            k_adjust = random.uniform(lowerB,upperB)\n",
    "            var_vector1 = copy.deepcopy(ref_vector1)*k_adjust\n",
    "            \n",
    "            for j in pos:\n",
    "                kk = random.uniform(lowerR,upperR)  \n",
    "                var_vector1[j] = ref_vector1[j]*kk*kk_sign[counter]\n",
    "                counter = counter + 1\n",
    "            zzz_x = np.concatenate((ref_vector1,var_vector1))   \n",
    "            zzz_y = np.array([1])\n",
    "            train_x = np.append(train_x,[zzz_x],axis=0)\n",
    "            train_y = np.append(train_y,[zzz_y],axis=0)\n",
    "\n",
    "        # benign lABEL 0\n",
    "        for icase in range(0,5):  # 5 cases here\n",
    "            pos = np.array(label_0_matrix[icase,:])\n",
    "            kk_sign = np.array(label_0_sign[icase,:])\n",
    "            counter = 0\n",
    "            \n",
    "            # Initial variant is obtained from deepcopy\n",
    "            k_adjust = random.uniform(lowerB,upperB)\n",
    "            var_vector1 = copy.deepcopy(ref_vector1)*k_adjust\n",
    "            \n",
    "            for j in pos:\n",
    "                kk = random.uniform(lowerR,upperR)\n",
    "                var_vector1[j] = ref_vector1[j]*kk*kk_sign[counter]\n",
    "                # print('-0- ', icase,j,kk,counter,kk_sign[counter])\n",
    "                counter = counter + 1\n",
    "            zzz_x = np.concatenate((ref_vector1,var_vector1))   \n",
    "            zzz_y = np.array([0])\n",
    "            train_x = np.append(train_x,[zzz_x],axis=0)\n",
    "            train_y = np.append(train_y,[zzz_y],axis=0)\n",
    "            \n",
    "        # benign lABEL 0 with \"similar\" variants (+/- 1%)\n",
    "        for icase in range(0,5):  # 5 cases here \n",
    "            # Initial variant is obtained from deepcopy\n",
    "            k_adjust = random.uniform(lowerB,upperB)\n",
    "            var_vector1 = copy.deepcopy(ref_vector1)*k_adjust\n",
    "            zzz_x = np.concatenate((ref_vector1,var_vector1))   \n",
    "            zzz_y = np.array([0])\n",
    "            train_x = np.append(train_x,[zzz_x],axis=0)\n",
    "            train_y = np.append(train_y,[zzz_y],axis=0)\n",
    "        # np.set_printoptions(precision=4)\n",
    "        # print(zzz)\n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_1_matrix.shape:  (10, 100)\n",
      "label_0_matrix.shape:  (10, 20)\n",
      "label_1_sign.shape:  (10, 100)\n",
      "label_0_sign.shape:  (10, 20)\n",
      "Generating the training dataset ...\n",
      "(2000, 2000)\n",
      "(2000, 1)\n"
     ]
    }
   ],
   "source": [
    "size_ref_vector = 1000\n",
    "\n",
    "ref_elements_to_change = int(size_ref_vector*0.1)  # 8%\n",
    "var_elements_to_change = int(size_ref_vector*0.02) # 2%\n",
    "\n",
    "label_1_matrix = np.random.randint(0,size_ref_vector-1, size=(10,ref_elements_to_change) )\n",
    "label_0_matrix = np.random.randint(0,size_ref_vector-1, size=(10,var_elements_to_change) )\n",
    "label_1_sign = np.random.choice([-1,1], size=(10,ref_elements_to_change) )\n",
    "label_0_sign = np.random.choice([-1,1], size=(10,var_elements_to_change) )\n",
    "\n",
    "print('label_1_matrix.shape: ' , label_1_matrix.shape) # (10, 100)\n",
    "print('label_0_matrix.shape: ' , label_0_matrix.shape) # (10, 20)\n",
    "print('label_1_sign.shape: ' , label_1_sign.shape) # (10, 100)\n",
    "print('label_0_sign.shape: ' , label_0_sign.shape) # (10, 20)\n",
    "\n",
    "Nset = 100 # 1 set of data = 20 samples, 100 set = 2000 samples \n",
    "print('Generating the training dataset ...')\n",
    "train_x, train_y=generate_dataset_new2(Nset,size_ref_vector,label_1_matrix,label_0_matrix,label_1_sign,label_0_sign)\n",
    "print(train_x.shape) # (2000, 2000)\n",
    "print(train_y.shape) # (2000, 1); input is 1x2000 which is  [ ref vector  variant vector ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the testing dataset ...\n",
      "(2000, 2000)\n",
      "(2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Each set of Test dataset is Nset x 80\n",
    "print('Generating the testing dataset ...')\n",
    "test_x, test_y=generate_dataset_new2(100,size_ref_vector,label_1_matrix,label_0_matrix,label_1_sign,label_0_sign)\n",
    "print(test_x.shape) # (2000, 2000)\n",
    "print(test_y.shape) # (2000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "27/27 - 0s - loss: 0.1589 - accuracy: 0.9594 - val_loss: 0.2049 - val_accuracy: 0.9400\n",
      "Epoch 2/50\n",
      "27/27 - 0s - loss: 0.0070 - accuracy: 0.9994 - val_loss: 0.1491 - val_accuracy: 0.9500\n",
      "Epoch 3/50\n",
      "27/27 - 0s - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0550 - val_accuracy: 0.9833\n",
      "Epoch 4/50\n",
      "27/27 - 0s - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.0409 - val_accuracy: 0.9867\n",
      "Epoch 5/50\n",
      "27/27 - 0s - loss: 8.4153e-04 - accuracy: 1.0000 - val_loss: 0.0161 - val_accuracy: 0.9933\n",
      "Epoch 6/50\n",
      "27/27 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "27/27 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "27/27 - 0s - loss: 9.3951e-04 - accuracy: 1.0000 - val_loss: 7.7027e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "27/27 - 0s - loss: 6.2903e-04 - accuracy: 1.0000 - val_loss: 5.2322e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "27/27 - 0s - loss: 7.1072e-04 - accuracy: 1.0000 - val_loss: 3.9793e-04 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "27/27 - 0s - loss: 5.5904e-04 - accuracy: 1.0000 - val_loss: 3.4674e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "27/27 - 0s - loss: 8.0027e-04 - accuracy: 1.0000 - val_loss: 3.1697e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "27/27 - 0s - loss: 7.2880e-04 - accuracy: 1.0000 - val_loss: 3.0035e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "27/27 - 0s - loss: 8.0218e-04 - accuracy: 1.0000 - val_loss: 2.8164e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "27/27 - 0s - loss: 0.0016 - accuracy: 0.9994 - val_loss: 2.7254e-04 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "27/27 - 0s - loss: 5.9684e-04 - accuracy: 1.0000 - val_loss: 2.6590e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "27/27 - 0s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 2.5305e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "27/27 - 0s - loss: 3.5853e-04 - accuracy: 1.0000 - val_loss: 2.4532e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "27/27 - 0s - loss: 4.5547e-04 - accuracy: 1.0000 - val_loss: 2.3621e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "27/27 - 0s - loss: 4.7315e-04 - accuracy: 1.0000 - val_loss: 2.2981e-04 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "27/27 - 0s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.0608e-04 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "27/27 - 0s - loss: 4.3235e-04 - accuracy: 1.0000 - val_loss: 1.8640e-04 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "27/27 - 0s - loss: 8.9192e-04 - accuracy: 1.0000 - val_loss: 1.7825e-04 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "27/27 - 0s - loss: 5.4595e-04 - accuracy: 1.0000 - val_loss: 1.7245e-04 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "27/27 - 0s - loss: 2.8604e-04 - accuracy: 1.0000 - val_loss: 1.6981e-04 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "27/27 - 0s - loss: 6.4615e-04 - accuracy: 1.0000 - val_loss: 1.6246e-04 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "27/27 - 0s - loss: 5.1762e-04 - accuracy: 1.0000 - val_loss: 1.5430e-04 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "27/27 - 0s - loss: 3.3628e-04 - accuracy: 1.0000 - val_loss: 1.4746e-04 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "27/27 - 0s - loss: 4.7261e-04 - accuracy: 1.0000 - val_loss: 1.4482e-04 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "27/27 - 0s - loss: 3.4805e-04 - accuracy: 1.0000 - val_loss: 1.3894e-04 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "27/27 - 0s - loss: 4.8951e-04 - accuracy: 1.0000 - val_loss: 1.3226e-04 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "27/27 - 0s - loss: 3.7783e-04 - accuracy: 1.0000 - val_loss: 1.2665e-04 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "27/27 - 0s - loss: 2.9204e-04 - accuracy: 1.0000 - val_loss: 1.2266e-04 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "27/27 - 0s - loss: 2.7633e-04 - accuracy: 1.0000 - val_loss: 1.1892e-04 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "27/27 - 0s - loss: 2.8826e-04 - accuracy: 1.0000 - val_loss: 1.1438e-04 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "27/27 - 0s - loss: 1.9110e-04 - accuracy: 1.0000 - val_loss: 1.1137e-04 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "27/27 - 0s - loss: 2.4176e-04 - accuracy: 1.0000 - val_loss: 1.0878e-04 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "27/27 - 0s - loss: 3.0818e-04 - accuracy: 1.0000 - val_loss: 1.0615e-04 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "27/27 - 0s - loss: 2.2335e-04 - accuracy: 1.0000 - val_loss: 1.0281e-04 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "27/27 - 0s - loss: 3.0186e-04 - accuracy: 1.0000 - val_loss: 9.9037e-05 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "27/27 - 0s - loss: 2.9236e-04 - accuracy: 1.0000 - val_loss: 9.5812e-05 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "27/27 - 0s - loss: 1.9063e-04 - accuracy: 1.0000 - val_loss: 9.2612e-05 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "27/27 - 0s - loss: 1.6504e-04 - accuracy: 1.0000 - val_loss: 9.0734e-05 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "27/27 - 0s - loss: 4.7143e-04 - accuracy: 1.0000 - val_loss: 9.0644e-05 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "27/27 - 0s - loss: 2.5882e-04 - accuracy: 1.0000 - val_loss: 8.6578e-05 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "27/27 - 0s - loss: 6.8218e-04 - accuracy: 1.0000 - val_loss: 8.2707e-05 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "27/27 - 0s - loss: 3.8852e-04 - accuracy: 1.0000 - val_loss: 7.4647e-05 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "27/27 - 0s - loss: 0.0023 - accuracy: 0.9994 - val_loss: 7.9357e-05 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "27/27 - 0s - loss: 3.1956e-04 - accuracy: 1.0000 - val_loss: 8.5900e-05 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "27/27 - 0s - loss: 2.3118e-04 - accuracy: 1.0000 - val_loss: 7.9802e-05 - val_accuracy: 1.0000\n",
      "Execution time:  9.302002429962158\n",
      "63/63 - 0s - loss: 0.0191 - accuracy: 0.9945\n",
      "Testing Accuracy =  [0.019052397459745407, 0.9944999814033508]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "history = ANN_model.fit(train_x, train_y, epochs=50, batch_size=64,validation_split=0.15,verbose = 2)\n",
    "end_time = time.time()\n",
    "exe_time = end_time - start_time\n",
    "print(\"Execution time: \", exe_time)\n",
    "scores = ANN_model.evaluate(test_x,test_y,verbose = 2)\n",
    "print(\"Testing Accuracy = \", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2000)\n",
      "(2000, 1)\n",
      "(2000, 1)\n",
      "** CHECK Average BCE Loss for multiple samples: [0.0191]\n",
      "Accuracy Score: 0.9945\n",
      "error0to1 =  10 ; label is 0\n",
      "error1to0 =  1 ; label is 1\n",
      "Testing total error =  11 percentError =  0.55\n",
      "2000 test cases: 1000 label 0; 1000 label 1\n"
     ]
    }
   ],
   "source": [
    "# To get the results of the ANN using test dataset\n",
    "y_pred = ANN_model.predict(test_x)\n",
    "\n",
    "epsilon = 1e-15  # Small value to prevent log(0)\n",
    "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "\n",
    "print(test_x.shape) # (1000, 2000)\n",
    "print(test_y.shape) # (1000, 1)\n",
    "print(y_pred.shape) # (1000, 1)\n",
    "\n",
    "total_loss = binary_cross_entropy_check(test_y, y_pred)\n",
    "print(f\"** CHECK Average BCE Loss for multiple samples: {total_loss}\")\n",
    "\n",
    "# To check on accuracy\n",
    "# first, convert the elements in y_pred so that negative becones epsilon, largest is 1 - epsilon\n",
    "epsilon = 1e-15  # Small value to prevent log(0)\n",
    "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "N=len(test_y)\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(test_y,y_pred_binary)\n",
    "print(\"Accuracy Score:\",accuracy)\n",
    "\n",
    "#Count error cases:\n",
    "#\n",
    "test_y_binary = test_y\n",
    "error0to1 = 0  # test_y is 0\n",
    "error1to0 = 0  # test_y is 1\n",
    "for i in range(N):\n",
    "    if (test_y_binary[i] == 0 and y_pred_binary[i] == 1):\n",
    "        error0to1 += 1\n",
    "    if (test_y_binary[i] == 1 and y_pred_binary[i] == 0):\n",
    "        error1to0 += 1\n",
    "        \n",
    "print('error0to1 = ',error0to1, '; label is 0') # test_y is 0, but predicted as 1\n",
    "print('error1to0 = ',error1to0, '; label is 1') # test_y is 1, but predicted as 0\n",
    "print('Testing total error = ',error0to1+error1to0,'percentError = ', 100*(error0to1+error1to0)/N) # Testing total error =  11 percentError =  0.55\n",
    "print('2000 test cases: 1000 label 0; 1000 label 1') # 2000 test cases: 1000 label 0; 1000 label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_x.shape  (2000, 2000)\n",
      "test_y.shape  (2000, 1)\n",
      "(2000, 1)\n",
      "Compare between test_y and y_pred_binary\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1]\n",
      "Sum of errors =  11   Percent Error 0.55\n"
     ]
    }
   ],
   "source": [
    "print('test_x.shape ', test_x.shape)\n",
    "print('test_y.shape ',test_y.shape)\n",
    "print(y_pred_binary.shape)\n",
    "\n",
    "print('Compare between test_y and y_pred_binary')\n",
    "N=len(test_y)\n",
    "\n",
    "errorcase=[0]*20\n",
    "for i in range(0,len(test_y)):\n",
    "    index = (i % 20)\n",
    "    if (test_y[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "print(errorcase)\n",
    "print('Sum of errors = ' ,np.sum(errorcase), '  Percent Error', 100*np.sum(errorcase)/N)  # sum of erorrs = 5, percent error (0.25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape  (2000, 2000)\n",
      "train_y.shape  (2000, 1)\n",
      "(2000, 1)\n",
      "Compare between train_y and y_pred_binary\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "len(error)  20\n",
      "Sum of errors =  0   Percent Error 0.0\n"
     ]
    }
   ],
   "source": [
    "print('train_x.shape ', train_x.shape)\n",
    "print('train_y.shape ',train_y.shape)\n",
    "\n",
    "# To get the results of the ANN using test dataset\n",
    "y_pred = ANN_model.predict(train_x)\n",
    "\n",
    "epsilon = 1e-15  # Small value to prevent log(0)\n",
    "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "\n",
    "N=len(train_y)\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "\n",
    "print(y_pred_binary.shape)\n",
    "print('Compare between train_y and y_pred_binary')\n",
    "\n",
    "errorcase=[0]*20\n",
    "for i in range(0,len(train_y)):\n",
    "    index = (i % 20)\n",
    "    if (train_y[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "print(errorcase)\n",
    "print('len(error) '  ,len(errorcase))\n",
    "print('Sum of errors = ' ,np.sum(errorcase), '  Percent Error', 100*np.sum(errorcase)/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat using the delta 'difference' vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1000) (2000, 1000) (2000, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Getting the difference vector of the training cases for training\n",
    "# input is now 1 x 1000; binary output\n",
    "ref_vector = train_x[:,0:1000]\n",
    "var_vector = train_x[:,1000:2000]\n",
    "diff_vector = np.abs(ref_vector - var_vector)\n",
    "print(ref_vector.shape,var_vector.shape,diff_vector.shape  ) # (2000, 1000) (2000, 1000) (2000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN model of the 1000 inputs; with halved number of neurons (smaller model!)\n",
    "def baseline_model4():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu', input_dim = 1000))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid')) # Output layer for binary classification\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "27/27 - 1s - loss: 0.1970 - accuracy: 0.9329 - val_loss: 0.4121 - val_accuracy: 0.9967\n",
      "Epoch 2/30\n",
      "27/27 - 0s - loss: 0.0277 - accuracy: 0.9994 - val_loss: 0.2967 - val_accuracy: 1.0000\n",
      "Epoch 3/30\n",
      "27/27 - 0s - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.2278 - val_accuracy: 1.0000\n",
      "Epoch 4/30\n",
      "27/27 - 0s - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.1705 - val_accuracy: 1.0000\n",
      "Epoch 5/30\n",
      "27/27 - 0s - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.1277 - val_accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "27/27 - 0s - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0919 - val_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "27/27 - 0s - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0630 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "27/27 - 0s - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0416 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "27/27 - 0s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0273 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "27/27 - 0s - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0173 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "27/27 - 0s - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "27/27 - 0s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "27/27 - 0s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0048 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "27/27 - 0s - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "27/27 - 0s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "27/27 - 0s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "27/27 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "27/27 - 0s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "27/27 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 8.9081e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "27/27 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 7.2359e-04 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "27/27 - 0s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 6.1106e-04 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "27/27 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 5.2644e-04 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "27/27 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 4.7428e-04 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "27/27 - 0s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.3634e-04 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "27/27 - 0s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.9088e-04 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "27/27 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.6110e-04 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "27/27 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.2378e-04 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "27/27 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.9747e-04 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "27/27 - 0s - loss: 9.9464e-04 - accuracy: 1.0000 - val_loss: 2.7926e-04 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "27/27 - 0s - loss: 9.8398e-04 - accuracy: 1.0000 - val_loss: 2.5912e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "ANN_model_diff = baseline_model4()\n",
    "history = ANN_model_diff.fit(diff_vector, train_y, epochs=30, batch_size=64,validation_split=0.15,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1000) (2000, 1000) (2000, 1000)\n",
      "63/63 - 0s - loss: 2.6855e-04 - accuracy: 1.0000\n",
      "0.0002685519284568727 1.0\n"
     ]
    }
   ],
   "source": [
    "# Results based on the vector difference of the 2000 test cases\n",
    "ref_vector_test = test_x[:,0:1000]\n",
    "var_vector_test = test_x[:,1000:2000]\n",
    "diff_vector_test = np.abs(ref_vector_test - var_vector_test)\n",
    "print(ref_vector_test.shape,var_vector_test.shape,diff_vector_test.shape  ) # (2000, 1000) (2000, 1000) (2000, 1000)\n",
    "\n",
    "# Evaluate on test set \n",
    "y_pred = ANN_model_diff.predict(diff_vector_test)\n",
    "\n",
    "loss_diff, acc_diff = ANN_model_diff.evaluate(diff_vector_test,test_y,verbose = 2) # 63/63 - 0s - loss: 2.6855e-04 - accuracy: 1.0000\n",
    "print(loss_diff, acc_diff) # 0.0002685519284568727 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DIR = \"path\"\n",
    "out_path = os.path.join(DIR, \"trainX_vectors3.csv\")\n",
    "print(out_path)\n",
    "np.savetxt(out_path,DBset,delimiter=',',fmt='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(DIR, \"trainY_vectors3.csv\")\n",
    "print(out_path)\n",
    "np.savetxt(out_path,DBy,delimiter=',',fmt='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = os.path.join(DIR, \"testX_vectors3.csv\")\n",
    "print(out_path)\n",
    "np.savetxt(out_path,DBset_test,delimiter=',',fmt='%.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO USE BELOW\n",
    "\n",
    "#Define ANOTHER ANN model\n",
    "input_dim = 16384\n",
    "ANNmodel2 = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=(1500,input_dim)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid') # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# compile with cross entropy\n",
    "ANNmodel2.compile(optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "start_time = time.time()\n",
    "history = ANNmodel2.fit(x_train, y_train, epochs=30, batch_size=32,verbose = 2)\n",
    "end_time = time.time()\n",
    "exe_time = end_time - start_time\n",
    "print(\"Execution time: \", exe_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ANNmodel2.evaluate(test_x,test_y,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Binary Cross-Entropy Loss (function): 0.20273661557656092\n",
      "--Average BCE Loss for multiple samples: 0.20273661557656092\n"
     ]
    }
   ],
   "source": [
    "# test code\n",
    "import numpy as np\n",
    "from keras.losses import binary_crossentropy\n",
    "import keras.backend as K\n",
    "\n",
    "# Example true labels and predicted probabilities\n",
    "y_true = np.array([0, 1, 1, 0, 1])\n",
    "y_pred = np.array([0.1, 0.9, 0.8, 0.2, 0.7])\n",
    "\n",
    "# Compute Binary Cross-Entropy using NumPy\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    bce = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return bce\n",
    "\n",
    "bce_loss = binary_cross_entropy(y_true, y_pred)\n",
    "print(f\"***Binary Cross-Entropy Loss (function): {bce_loss}\")\n",
    "\n",
    "#===========================================================================\n",
    "def binary_cross_entropy_np(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates Binary Cross-Entropy loss for multiple samples using NumPy.\n",
    "    y_true: NumPy array of actual labels (0s and 1s)\n",
    "    y_pred: NumPy array of predicted probabilities (between 0 and 1)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # Small value to prevent log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "total_loss = binary_cross_entropy_np(y_true, y_pred)\n",
    "print(f\"--Average BCE Loss for multiple samples: {total_loss}\")\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "# Compute Binary Cross-Entropy using Keras\n",
    "# DOES NOT WORK\n",
    "# bce_loss_keras = binary_crossentropy(K.constant(y_true), K.constant(y_pred)).numpy()\n",
    "# print(f\"Binary Cross-Entropy Loss (Keras): {bce_loss_keras}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4\n",
      "-2.5\n"
     ]
    }
   ],
   "source": [
    "aaa = np.array([0, 1, 1, -2.5, 1 , 3.4])\n",
    "bbb = np.array([0.1, 0.9, 0.8, 0.2, 0.7])\n",
    "\n",
    "print(max(aaa))\n",
    "print(min(aaa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    DBset = np.empty((0,4))\n",
    "    DBset_tmp = np.array([1,2,3,4])\n",
    "    DBset = np.append(DBset,[DBset_tmp],axis=0)\n",
    "    print(DBset)\n",
    "    \n",
    "    DBset_tmp = np.array([5,6,7,8])\n",
    "    DBset = np.append(DBset,[DBset_tmp],axis=0)\n",
    "\n",
    "    print(DBset)\n",
    "    print('========================')\n",
    "    DBset_tmp = np.array([9,10,11,12])\n",
    "    DBset = np.append(DBset,[DBset_tmp],axis=0)\n",
    "\n",
    "    print(DBset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO LOAD / READIN\n",
    "\n",
    "# READ IN CSV (takes 30 seconds)\n",
    "new_train_values = np.loadtxt(\"train_vectors2.csv\",delimiter=\",\")\n",
    "print(new_train_values.shape)\n",
    "new_train = new_train_values.reshape((4000,16385))\n",
    "print('new_train.shape = ',new_train.shape)\n",
    "\n",
    "\n",
    "new_test_values = np.loadtxt(\"test_vectors2.csv\",delimiter=\",\")\n",
    "print(new_test_values.shape)\n",
    "new_test = new_test_values.reshape((4000,16385))\n",
    "print('new_test.shape = ',new_test.shape)\n",
    "\n",
    "train_x = new_train[:,0:16384] \n",
    "train_y = new_train[:,16384]\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "#===================\n",
    "test_x = new_test[:,0:16384] \n",
    "test_y = new_test[:,16384]\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
