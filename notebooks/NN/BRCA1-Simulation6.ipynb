{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN Training\n",
    "import sys\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE= 8192 # 8192\n",
    "MODEL_SIZE=\"7b\"\n",
    "SUBSET_METHOD=\"all\" # \"random, top, bottom, balanced, all\"\n",
    "REGION = \"BRCA1_DATA\" # BRCA1_DATA, RovHer_BRCA1 or RovHer_LDLR, \"both\"\n",
    "LAYER=\"blocks.28.mlp.l3\"\n",
    "COMBO=\"delta\"\n",
    "\n",
    "# Directories\n",
    "INPUT_DIR = Path(\"/mnt/nfs/rigenenfs/shared_resources/biobanks/UKBIOBANK/pangk/evo2/BRCA1_LDLR\")\n",
    "INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUT_DIR = Path(f\"/mnt/nfs/rigenenfs/shared_resources/biobanks/UKBIOBANK/pangk/evo2/NN/BRCA1_LDLR_{COMBO}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Input data: \n",
    "delta_file = f\"{INPUT_DIR}/{REGION}_{LAYER}_delta.csv\"\n",
    "delta_rev_file = f\"{INPUT_DIR}/{REGION}_{LAYER}_delta_rev.csv\"\n",
    "ref_file = f\"{INPUT_DIR}/{REGION}_{LAYER}_ref.csv\"\n",
    "var_file = f\"{INPUT_DIR}/{REGION}_{LAYER}_var.csv\"\n",
    "ref_rev_file = f\"{INPUT_DIR}/{REGION}_{LAYER}_ref_rev.csv\"\n",
    "var_rev_file = f\"{INPUT_DIR}/{REGION}_{LAYER}_var_rev.csv\"\n",
    "\n",
    "if REGION == \"BRCA1_DATA\":\n",
    "    file = \"/mnt/nfs/rigenenfs/workspace/pangk/Softwares/evo2/data/BRCA1_DATA.xlsx\" # training variants + labels\n",
    "else:\n",
    "    DIR=\"/mnt/nfs/rigenenfs/shared_resources/biobanks/UKBIOBANK/pangk\"\n",
    "    label_file1 = f\"{DIR}/RARity_monogenic_benchmark/BRCAexchange/BRCA1_clinvar_cleaned.txt\" \n",
    "    label_file2 = f\"{DIR}/RARity_monogenic_benchmark/LOVD_LDLR/LDLR_clinvar_curated.txt\" # British heart foundation-classified variants on LOVD\n",
    "\n",
    "if REGION == \"both\":\n",
    "    REGION = \"RovHer_BRCA1\" \n",
    "    ref_file1 = f\"{INPUT_DIR}/{REGION}_{LAYER}_ref.csv\"\n",
    "    var_file1 = f\"{INPUT_DIR}/{REGION}_{LAYER}_var.csv\"\n",
    "    ref_rev_file1 = f\"{INPUT_DIR}/{REGION}_{LAYER}_ref_rev.csv\"\n",
    "    var_rev_file1 = f\"{INPUT_DIR}/{REGION}_{LAYER}_var_rev.csv\"\n",
    "    REGION = \"RovHer_LDLR\" \n",
    "    ref_file2 = f\"{INPUT_DIR}/{REGION}_{LAYER}_ref.csv\"\n",
    "    var_file2 = f\"{INPUT_DIR}/{REGION}_{LAYER}_var.csv\"\n",
    "    ref_rev_file2 = f\"{INPUT_DIR}/{REGION}_{LAYER}_ref_rev.csv\"\n",
    "    var_rev_file2 = f\"{INPUT_DIR}/{REGION}_{LAYER}_var_rev.csv\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "def sample_data(df, sample_frac=1.0, balanced=True, disable=True, random_state=42):\n",
    "    \"\"\"Sample dataframe, optionally with balanced classes.\n",
    "    \"\"\"\n",
    "    if disable:\n",
    "        return df\n",
    "    if balanced: # Get the number of rows in the dataframe\n",
    "        num_rows_minor_class = math.ceil(len(df[df[\"class\"] == \"LOF\"]) * sample_frac)\n",
    "        return (\n",
    "            pd.concat(\n",
    "                [\n",
    "                    df[df[\"class\"] == \"LOF\"].sample(n=num_rows_minor_class, random_state=random_state),\n",
    "                    df[df[\"class\"] == \"FUNC/INT\"].sample(n=num_rows_minor_class, random_state=random_state),\n",
    "                ]\n",
    "            )\n",
    "            .sample(frac=1.0, random_state=random_state)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    else: # Calculate the number of rows to sample\n",
    "        return df.sample(frac=sample_frac, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "def subset_dataframe(df, seq):\n",
    "    \"\"\"\n",
    "    Randomly subsets the dataframe to SEQ_LENGTH number of rows.\n",
    "    Returns: pandas.DataFrame - A subset of the dataframe with SEQ_LENGTH rows.\n",
    "    \"\"\"\n",
    "    print(\"Number of rows to extract:\", seq) \n",
    "    if seq > len(df):\n",
    "        raise ValueError(f\"SEQ_LENGTH ({seq}) is greater than the number of rows in the DataFrame ({len(df)}).\")\n",
    "    subset_df = df.sample(n=seq, random_state=42)\n",
    "    print(\"New subset:\", subset_df.shape) \n",
    "    return subset_df\n",
    "\n",
    "def parse_sequences(pos, ref, alt, refseq, window_size=WINDOW_SIZE):\n",
    "    \"\"\"Parse reference and variant sequences from the reference genome sequence.\n",
    "    Returns:  tuple (reference_sequence, variant_sequence)\n",
    "    \"\"\"\n",
    "    p = pos - 1  # Convert to 0-indexed position\n",
    "    full_seq = refseq\n",
    "    ref_seq_start = max(0, p - window_size // 2)\n",
    "    ref_seq_end = min(len(full_seq), p + window_size // 2)\n",
    "    ref_seq = refseq[ref_seq_start:ref_seq_end]\n",
    "    snv_pos_in_ref = min(window_size // 2, p)\n",
    "    var_seq = ref_seq[:snv_pos_in_ref] + alt + ref_seq[snv_pos_in_ref + 1 :]\n",
    "    # Sanity checks\n",
    "    assert len(var_seq) == len(ref_seq)\n",
    "    assert ref_seq[snv_pos_in_ref] == ref\n",
    "    assert var_seq[snv_pos_in_ref] == alt\n",
    "    return ref_seq, var_seq\n",
    "\n",
    "def generate_fasta_files(df, refseq, output_dir=FASTA_DIR, window_size=WINDOW_SIZE):\n",
    "    \"\"\"Generate FASTA files for ref and var sequences.\n",
    "    Returns:pandas.DataFrame with added columns for FASTA names\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Paths for output files\n",
    "    ref_fasta_path = output_dir / \"reference_sequences.fasta\"\n",
    "    var_fasta_path = output_dir / \"variant_sequences.fasta\"\n",
    "    # Track unique sequences\n",
    "    ref_sequences = set()\n",
    "    var_sequences = set()\n",
    "    ref_seq_to_name = {}\n",
    "    # Store unique sequences with metadata for writing\n",
    "    ref_entries = []\n",
    "    var_entries = []\n",
    "    ref_names = []\n",
    "    var_names = []\n",
    "    # Collect unique reference and variant sequences\n",
    "    for idx, row in df.iterrows():\n",
    "        ref_seq, var_seq = parse_sequences(row[\"pos\"], row[\"ref\"], row[\"alt\"], refseq, window_size)\n",
    "        # Add to sets to ensure uniqueness\n",
    "        if ref_seq not in ref_sequences:\n",
    "            ref_sequences.add(ref_seq)\n",
    "            ref_name = f\"ref_pos_{row['pos']}_{row['ref']}\"\n",
    "            ref_entries.append(f\">{ref_name}\\n{ref_seq}\\n\")\n",
    "            ref_names.append(ref_name)\n",
    "            ref_seq_to_name[ref_seq] = ref_name\n",
    "        else:\n",
    "            ref_name = ref_seq_to_name[ref_seq]\n",
    "            ref_names.append(ref_name)\n",
    "        if var_seq not in var_sequences:\n",
    "            var_sequences.add(var_seq)\n",
    "            var_name = f\"var_pos_{row['pos']}_{row['ref']}to{row['alt']}\"\n",
    "            var_entries.append(f\">{var_name}\\n{var_seq}\\n\")\n",
    "            var_names.append(var_name)\n",
    "        else:\n",
    "            assert False, \"Duplicate variant sequence\"\n",
    "    # Write unique sequences to FASTA files\n",
    "    with open(ref_fasta_path, \"w\") as f:\n",
    "        f.writelines(ref_entries)\n",
    "    with open(var_fasta_path, \"w\") as f:\n",
    "        f.writelines(var_entries)\n",
    "    # Add FASTA names to dataframe\n",
    "    df_with_names = df.copy()\n",
    "    df_with_names[\"ref_fasta_name\"] = ref_names\n",
    "    df_with_names[\"var_fasta_name\"] = var_names\n",
    "    print(f\"Unique reference sequences: {len(ref_sequences)}\")\n",
    "    print(f\"Unique variant sequences: {len(var_sequences)}\")\n",
    "    return df_with_names\n",
    "\n",
    "# Compute Binary Cross-Entropy using NumPy\n",
    "def binary_cross_entropy_np(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates Binary Cross-Entropy loss for multiple samples using NumPy.\n",
    "    y_true: NumPy array of actual labels (0s and 1s)\n",
    "    y_pred: NumPy array of predicted probabilities (between 0 and 1)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # Small value to prevent log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "# Compute Binary Cross-Entropy using own calculaton for checking\n",
    "def binary_cross_entropy_check(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates Binary Cross-Entropy loss for multiple samples using NumPy.\n",
    "    y_true: NumPy array of actual labels (0s and 1s)\n",
    "    y_pred: NumPy array of predicted probabilities (between 0 and 1)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # Small value to prevent log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "    N=len(y_true)\n",
    "    BCE_sum=0\n",
    "    for i in range(N):\n",
    "        BCE_sum = BCE_sum + y_true[i]*np.log(y_pred[i]) + (1-y_true[i])*np.log((1-y_pred[i])) \n",
    "    BCE = (-1*BCE_sum)/N\n",
    "    return BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training labels \n",
    "\n",
    "The *BRCA1* SNV dataset was obtained from [Findlay et al. (2018)](https://www.nature.com/articles/s41586-018-0461-z), which contains 3,893 SNVs. Among them, 631 SNVs have ClinVar classification, which we recoded as numerical labels [0, 0.25, 0.5, 0.75, 1] for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recode_clinvar(value):\n",
    "    mapping = {\n",
    "        \"P\": 1,\n",
    "        \"B\": 0,\n",
    "        \"LB\": 0.25,\n",
    "        \"LP\": 0.75,\n",
    "        \"LP,P\": 0.75,\n",
    "        \"B/LB\": 0.25\n",
    "    }\n",
    "    return mapping.get(value, 0.5)\n",
    "\n",
    "recode_map = {\n",
    "    \"Pathogenic\": 1,\n",
    "    'Pathogenic/Likely pathogenic': 1,\n",
    "    'Likely pathogenic': 0.75,\n",
    "    \"Uncertain significance\": 0.5,\n",
    "    \"Likely benign\": 0.25,\n",
    "    \"Benign\": 0,\n",
    "    \"absent\": \"NA\",\n",
    "    'Conflicting interpretations of pathogenicity': \"NA\",\n",
    "}\n",
    "\n",
    "# 1. Variant data + ClinVar labels \n",
    "if REGION == \"BRCA1_DATA\":\n",
    "    data = pd.read_excel(file, header=2)\n",
    "    data = data[['chromosome', 'position (hg19)', 'reference', 'alt', 'function.score.mean', 'func.class', 'clinvar',]]\n",
    "    data.rename(columns={\n",
    "            'chromosome': 'chrom','position (hg19)': 'pos',\n",
    "            'reference': 'ref','alt': 'alt',\n",
    "            'function.score.mean': 'score','func.class': 'class', 'clinvar': 'clinvar',\n",
    "        }, inplace=True)\n",
    "    # Re-code values \n",
    "    data['class'] = data['class'].replace(['FUNC', 'INT'], 'FUNC/INT')\n",
    "    # Create new column \n",
    "    data['PLINK_SNP_NAME'] = data.apply(\n",
    "            lambda row: f\"{row['chrom']}:{row['pos']}:{row['ref']}:{row['alt']}\", axis=1\n",
    "    )\n",
    "    # Recode `clinvar` column\n",
    "    unique_clinvar_values = data['clinvar'].unique()\n",
    "    print(\"Unique values in clinvar column:\", unique_clinvar_values)\n",
    "    data['clinvar'] = data['clinvar'].replace(recode_map)\n",
    "    lof_count = data[data[\"class\"] == \"LOF\"].shape[0]\n",
    "    print(f\"Test 'LOF': {lof_count}\\n\")\n",
    "    other_count = data[data[\"class\"] == \"FUNC/INT\"].shape[0]\n",
    "    print(f\"Test 'FUNC/INT': {other_count}\\n\")\n",
    "else:\n",
    "    # BRCA1\n",
    "    ACMG_col1 = pd.read_csv(label_file1, sep=\"\\t\", usecols=[\"PLINK_SNP_NAME\", \"ACMG_final\"])\n",
    "    ACMG_col1 = ACMG_col1.rename(columns={\"ACMG_final\": \"clinvar\"})\n",
    "    # LDLR\n",
    "    ACMG_col2 = pd.read_csv(label_file2, sep=\"\\t\", usecols=[\"PLINK_SNP_NAME\", \"clinvar_clnsig\"])\n",
    "    ACMG_col2 = ACMG_col2.rename(columns={\"clinvar_clnsig\": \"clinvar\"})\n",
    "    # Combine \n",
    "    data = pd.concat([ACMG_col1, ACMG_col2], ignore_index=True)\n",
    "    print(f\"BRCA1 and LDLR merged: {data.shape}\")\n",
    "    # (883, 2)\n",
    "    data = data[~data[\"clinvar\"].isin([\"\", \"NA\", \"CCP\"])]\n",
    "    data[\"clinvar\"] = data[\"clinvar\"].apply(recode_clinvar)\n",
    "    print(data[\"clinvar\"].value_counts(dropna=False))\n",
    "\n",
    "# Remove rows with missing clinvar anno\n",
    "data = data[data['clinvar'] != \"NA\"]\n",
    "print(\"After removing NA in clinvar:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Evo2 7B model emeddings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMBO == \"delta\":\n",
    "    # Variant + reverse complement embeddings \n",
    "    delta = pd.read_csv(delta_file)\n",
    "    delta_reverse = pd.read_csv(delta_rev_file)\n",
    "\n",
    "if COMBO == \"refvar\":\n",
    "    if REGION == \"BRCA1_DATA\" or REGION == \"RovHer_BRCA1\" or REGION == \"RovHer_LDLR\":\n",
    "        # 1. Variant + reverse complement \n",
    "        var = pd.read_csv(var_file)\n",
    "        var_reverse = pd.read_csv(var_rev_file)\n",
    "        # 2. Reference + reverse complement\n",
    "        ref = pd.read_csv(ref_file)\n",
    "        ref_reverse = pd.read_csv(ref_rev_file)\n",
    "\n",
    "    if REGION == \"both\":\n",
    "        var1 = pd.read_csv(var_file1)\n",
    "        var_reverse1 = pd.read_csv(var_rev_file1)\n",
    "        ref1 = pd.read_csv(ref_file1)\n",
    "        ref_reverse1 = pd.read_csv(ref_rev_file1)\n",
    "        var2 = pd.read_csv(var_file2)\n",
    "        var_reverse2 = pd.read_csv(var_rev_file2)\n",
    "        ref2 = pd.read_csv(ref_file2)\n",
    "        ref_reverse2 = pd.read_csv(ref_rev_file2)\n",
    "\n",
    "        var = pd.concat([var1, var2], ignore_index=True)\n",
    "        var_reverse = pd.concat([var_reverse1, var_reverse2], ignore_index=True)\n",
    "        ref = pd.concat([ref1, ref2], ignore_index=True)\n",
    "        ref_reverse = pd.concat([ref_reverse1, ref_reverse2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset rows from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows based on the PLINK_SNP_NAME column\n",
    "data = data[~data['PLINK_SNP_NAME'].duplicated(keep='first')]\n",
    "\n",
    "if COMBO == \"delta\":\n",
    "    # Step 1: Compute the strict intersection of PLINK_SNP_NAME across all dfs\n",
    "    final_common_snp_names = list(\n",
    "        set(data['PLINK_SNP_NAME'])\n",
    "        .intersection(delta['PLINK_SNP_NAME'])\n",
    "        .intersection(delta_reverse['PLINK_SNP_NAME'])\n",
    "    )\n",
    "    # Step 2: Filter all dfs simultaneously based on the common SNP names\n",
    "    data = data[data['PLINK_SNP_NAME'].isin(final_common_snp_names)].reset_index(drop=True)\n",
    "    delta = delta[delta['PLINK_SNP_NAME'].isin(final_common_snp_names)].reset_index(drop=True)\n",
    "    delta_reverse = delta_reverse[delta_reverse['PLINK_SNP_NAME'].isin(final_common_snp_names)].reset_index(drop=True)\n",
    "    # Tallies\n",
    "    print(\"Filtered labels file (data):\", data.shape)\n",
    "    print(\"delta:\", delta.shape, \"delta_reverse:\", delta_reverse.shape)\n",
    "    # Check if the number of rows match\n",
    "    if not (delta.shape[0] == data.shape[0] and\n",
    "            delta_reverse.shape[0] == data.shape[0]):\n",
    "        raise ValueError(\"Number of rows in embeddings do not match number of rows in data.\")\n",
    "\n",
    "\n",
    "if COMBO == \"refvar\":\n",
    "    # Step 1: Compute the strict intersection of PLINK_SNP_NAME across all dfs\n",
    "    final_common_snp_names = list(\n",
    "        set(data['PLINK_SNP_NAME'])\n",
    "        .intersection(var['PLINK_SNP_NAME'])\n",
    "        .intersection(var_reverse['PLINK_SNP_NAME'])\n",
    "        .intersection(ref['PLINK_SNP_NAME'])\n",
    "        .intersection(ref_reverse['PLINK_SNP_NAME'])\n",
    "    )\n",
    "    # Step 2: Filter all dfs based on the common SNP names\n",
    "    data = data[data['PLINK_SNP_NAME'].isin(final_common_snp_names)].reset_index(drop=True)\n",
    "    var = var[var['PLINK_SNP_NAME'].isin(final_common_snp_names)].reset_index(drop=True)\n",
    "    var_reverse = var_reverse[var_reverse['PLINK_SNP_NAME'].isin(final_common_snp_names)].reset_index(drop=True)\n",
    "    ref = ref[ref['PLINK_SNP_NAME'].isin(final_common_snp_names)].reset_index(drop=True)\n",
    "    ref_reverse = ref_reverse[ref_reverse['PLINK_SNP_NAME'].isin(final_common_snp_names)].reset_index(drop=True)\n",
    "\n",
    "    # Tallies\n",
    "    print(\"Filtered labels file (data):\", data.shape)\n",
    "    print(\"var:\", var.shape, \"var_reverse:\", var_reverse.shape, \"ref:\", ref.shape, \"ref_reverse:\", ref_reverse.shape)\n",
    "\n",
    "    # Check if the number of rows match\n",
    "    if not (var.shape[0] == data.shape[0] and\n",
    "            var_reverse.shape[0] == data.shape[0] and\n",
    "            ref.shape[0] == data.shape[0] and\n",
    "            ref_reverse.shape[0] == data.shape[0]):\n",
    "        raise ValueError(\"Number of rows in embeddings do not match number of rows in data.\")\n",
    "\n",
    "print(\"Counts of unique values in clinvar column:\")\n",
    "print(data['clinvar'].value_counts())\n",
    "numeric_rows = data['clinvar'].apply(lambda x: isinstance(x, (int, float))).sum()\n",
    "print(\"Number of clinvar annotations:\", numeric_rows, \"of\", data.shape[0], \"rows\") #  631 of 3893 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset columns from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'input_file' and 'layer' columns\n",
    "\n",
    "if COMBO == \"delta\":\n",
    "    delta = delta.drop(columns=['PLINK_SNP_NAME','input_file', 'layer'])\n",
    "    delta_reverse = delta_reverse.drop(columns=['PLINK_SNP_NAME','input_file', 'layer'])\n",
    "    print(f\"Variant embeddings: {delta.shape}\")\n",
    "    print(f\"Variant reverse comp. embeddings: {delta_reverse.shape}\")\n",
    "\n",
    "if COMBO == \"refvar\":\n",
    "    var = var.drop(columns=['PLINK_SNP_NAME','input_file', 'layer'])\n",
    "    var_reverse = var_reverse.drop(columns=['PLINK_SNP_NAME','input_file', 'layer'])\n",
    "    ref = ref.drop(columns=['PLINK_SNP_NAME','input_file', 'layer'])\n",
    "    ref_reverse = ref_reverse.drop(columns=['PLINK_SNP_NAME','input_file', 'layer'])\n",
    "    print(f\"Variant embeddings: {var.shape}\") # (631, 4096)\n",
    "    print(f\"Variant reverse comp. embeddings: {var_reverse.shape}\") # (631, 4096)\n",
    "    print(f\"Reference embeddings: {ref.shape}\") # (631, 4096)\n",
    "    print(f\"Reference reverse comp. embeddings: {ref_reverse.shape}\") # (631, 4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build feature vector by concatenation\n",
    "vector = [reference + reference reverse complement + variant + variant reverse complement]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMBO == \"delta\":\n",
    "    # feature vector for each SNV  (631, 8192)\n",
    "    feature_vec = np.hstack([\n",
    "        delta.values,         # delta embeddings\n",
    "        delta_reverse.values, # Reverse complement\n",
    "    ])\n",
    "    \n",
    "if COMBO == \"refvar\":\n",
    "    # feature vector for each SNV (3893, 16384) | 16384 features per SNV\n",
    "    feature_vec = np.hstack([\n",
    "        ref.values,         # Reference embeddings\n",
    "        ref_reverse.values, # Reverse complement of reference\n",
    "        var.values,         # Variant embeddings\n",
    "        var_reverse.values  # Reverse complement of variant\n",
    "    ])\n",
    "\n",
    "print(f\"feature_vec embeddings: {feature_vec.shape}\")\n",
    "\n",
    "# Extract labels\n",
    "train_y = data['clinvar'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset\n",
    "1. Training Set: The remaining 80% of the training data.\n",
    "2. Test Set: 20% of the data (withheld entirely from training).\n",
    "3. Validation Set: 20% of the remaining training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all of X_train for training (more training data), w/ internal validation_split during training\n",
    "internal_validation_split=\"no\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if internal_validation_split == \"yes\":\n",
    "    # Split dataset into test (20%) and remaining training data (80%)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        feature_vec, train_y, test_size=0.2, random_state=42, stratify=train_y\n",
    "    )\n",
    "    print(f\"Training set size: {X_train.shape}\")\n",
    "else:\n",
    "    # Check if REGION requires filtering of classes\n",
    "    if REGION == \"RovHer_LDLR\":\n",
    "        # Check class distribution and remove classes with fewer than 2 instances\n",
    "        class_counts = pd.Series(train_y).value_counts()\n",
    "        print(\"Class distribution before filtering:\")\n",
    "        print(class_counts)\n",
    "        # Keep only classes with at least 2 instances\n",
    "        classes_to_keep = class_counts[class_counts >= 2].index\n",
    "        mask = np.isin(train_y, classes_to_keep)\n",
    "        feature_vec = feature_vec[mask]\n",
    "        train_y = train_y[mask]\n",
    "        # Check class distribution again after filtering\n",
    "        print(\"Class distribution after filtering:\")\n",
    "        print(pd.Series(train_y).value_counts())\n",
    "    # MANUALLY split dataset into test (20%) and remaining training data (80%)\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        feature_vec, train_y, test_size=0.2, random_state=42, stratify=train_y\n",
    "    )\n",
    "    # Ensure no class has fewer than 2 instances in the remaining training data\n",
    "    class_counts_val = pd.Series(y_train_val).value_counts()\n",
    "    if (class_counts_val < 2).any():\n",
    "        print(\"Warning: Some classes in y_train_val have fewer than 2 instances. Adjusting...\")\n",
    "        # Filter out classes with fewer than 2 instances in y_train_val\n",
    "        classes_to_keep_val = class_counts_val[class_counts_val >= 2].index\n",
    "        mask_val = np.isin(y_train_val, classes_to_keep_val)\n",
    "        X_train_val = X_train_val[mask_val]\n",
    "        y_train_val = y_train_val[mask_val]\n",
    "    # Split the remaining training data into train (80%) and validation (20%)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.2, random_state=42, stratify=y_train_val\n",
    "    )\n",
    "    # Convert validation and training data to float32\n",
    "    X_val = X_val.astype('float32')\n",
    "    y_val = y_val.astype('float32')\n",
    "    X_train_val = X_train_val.astype('float32')\n",
    "    y_train_val = y_train_val.astype('float32')\n",
    "    print(f\"Validation set size: {X_val.shape}\")\n",
    "\n",
    "# if internal_validation_split == \"yes\":\n",
    "#     # Split dataset into test (20%) and remaining training data (80%)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         feature_vec, train_y, test_size=0.2, random_state=42, stratify=train_y\n",
    "#     )\n",
    "#     print(f\"Training set size: {X_train.shape}\") \n",
    "# else: \n",
    "#     if REGION == \"RovHer_LDLR\":\n",
    "#         # Check class distribution, Remove those with fewer than 2 instances\n",
    "#         class_counts = pd.Series(train_y).value_counts()\n",
    "#         classes_to_keep = class_counts[class_counts >= 2].index\n",
    "#         mask = np.isin(train_y, classes_to_keep)\n",
    "#         feature_vec = feature_vec[mask]\n",
    "#         train_y = train_y[mask]\n",
    "#     # MANUALLY split into test (20%) and remaining training data (80%)\n",
    "#     X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "#         feature_vec, train_y, test_size=0.2, random_state=42, stratify=train_y\n",
    "#     )\n",
    "#     # Split remaining training data into train (80%) and validation (20%)\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(\n",
    "#         X_train_val, y_train_val, test_size=0.2, random_state=42, stratify=y_train_val\n",
    "#     )\n",
    "#     X_val = X_val.astype('float32')\n",
    "#     y_val = y_val.astype('float32')\n",
    "#     X_train_val = X_train_val.astype('float32')\n",
    "#     y_train_val = y_train_val.astype('float32')\n",
    "#     print(f\"Validation set size: {X_val.shape}\")\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\") # (403, 16384)\n",
    "print(f\"Test set size: {X_test.shape}\") # (127, 16384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ANN\n",
    "* Input Layer = 32,768 features.\n",
    "* Hidden Layers: 512 → 128 → 32 neurons.\n",
    "* Output Layer: Binary classification (pathogenic probability).\n",
    "* Activation: ReLU for hidden layers, Sigmoid for the output layer.\n",
    "* Batch Normalization and Dropout (𝑝=0.3) after each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ANN model, with output layer for binary classification\n",
    "input_dim = feature_vec.shape[1]\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu',input_dim=input_dim))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['AUC'])\n",
    "    return model\n",
    "\n",
    "ANN_model = build_model()\n",
    "ANN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# reserves 15% of the training data (X_train and y_train) for validation during training\n",
    "if internal_validation_split == \"yes\":\n",
    "    history = ANN_model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=100, \n",
    "        batch_size=64, \n",
    "        validation_split=0.15, \n",
    "    )\n",
    "    print(\"Internal validation split enabled during training\\n\")\n",
    "else:\n",
    "    history = ANN_model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=100, \n",
    "        batch_size=64, \n",
    "        verbose=2\n",
    "    )\n",
    "    print(\"No internal validation split occured.\\n\")\n",
    "\n",
    "end_time = time.time()\n",
    "exe_time = end_time - start_time\n",
    "print(\"Training time: \", exe_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test and validation sets\n",
    "* validation set- performance on unseen data, monitor overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_auc = ANN_model.evaluate(X_test, y_test, verbose=2)\n",
    "print(f\"Test Loss: {test_loss:.4f} | AUC: {test_auc:.4f}\\n\") # Test Loss: 0.7120 | AUC: 0.8740\n",
    "\n",
    "# Evaluate on the validation set\n",
    "if internal_validation_split == \"no\":  \n",
    "    val_loss, val_auc = ANN_model.evaluate(X_val, y_val, verbose=2)\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training loss & AUC\n",
    "* Loss vs Epochs and AUC vs Epochs (Training)\n",
    "* ROC curve (Test and Validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation AUC\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['auc'], label='Training AUC')\n",
    "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "plt.title('Training AUC')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/{REGION}_{LAYER}_train_AUC_loss.png\")\n",
    "print(\"Loss plot:\", f\"{OUTPUT_DIR}/{REGION}_{LAYER}_train_AUC_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary thresholding for AUC calculation\n",
    "threshold = 0.5\n",
    "\n",
    "# Set up a single figure for side-by-side subplots\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Test Set ROC Curve\n",
    "plt.subplot(1, 2, 1)  # First subplot (1 row, 2 columns, 1st plot)\n",
    "y_test_binary = np.where(y_test > threshold, 1, 0)\n",
    "y_test_pred_prob = ANN_model.predict(X_test).ravel()  # Flatten predictions\n",
    "fpr, tpr, thresholds = roc_curve(y_test_binary, y_test_pred_prob)\n",
    "auc = roc_auc_score(y_test_binary, y_test_pred_prob)\n",
    "print(f\"AUC (binary thresholding - Test Set): {auc:.4f}\")\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc:.4f}\", color=\"blue\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Test Set ROC Curve (Binary-thresholded AUC)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot Validation Set ROC Curve\n",
    "plt.subplot(1, 2, 2)  # Second subplot (1 row, 2 columns, 2nd plot)\n",
    "y_val_binary = np.where(y_val > threshold, 1, 0)\n",
    "y_val_pred_prob = ANN_model.predict(X_val).ravel()  # Flatten predictions\n",
    "fpr_val, tpr_val, thresholds_val = roc_curve(y_val_binary, y_val_pred_prob)\n",
    "auc_val = roc_auc_score(y_val_binary, y_val_pred_prob)\n",
    "print(f\"AUC (binary thresholding - Validation Set): {auc_val:.4f}\")\n",
    "plt.plot(fpr_val, tpr_val, label=f\"AUC = {auc_val:.4f}\", color=\"green\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Validation Set ROC Curve (Binary-thresholded AUC)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/{REGION}_{LAYER}_ROC.png\")\n",
    "plt.show()\n",
    "print(f\"AUC (Test): {auc:.4f}  (Validation): {auc_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is 1x2000 which is  [ ref vector  variant vector ]\n",
    "\n",
    "# The training dataset (balanced) is as follows:\n",
    "#  [one_ref_vector  variant_vector_1 (8% or 80 locations modified)]    label 1 : 10 cases\n",
    "#  [one_ref_vector  variant_vector_2 (2% or 20 locations modified)]    label 0 : 5 cases\n",
    "#  [one_ref_vector  variant_vector_3 (similar to one_ref_vector)  ]    label 0 : 5 cases\n",
    "#\n",
    "# Based on the search, the benign variants is designed to have changes of 2% of elements.(label 0)\n",
    "# The pathogenic variants have a higher frequency of alterations of 8% of elements. (label 1)\n",
    "#\n",
    "# generate_dataset_new2() is the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the dataset\n",
    "# Ref vector is 1x1000; variant vector is 1x1000\n",
    "def generate_dataset_new2(num_items,size_ref_vector,label_1_matrix,label_0_matrix,label_1_sign,label_0_sign):\n",
    "    train_x = np.empty((0,2*size_ref_vector))\n",
    "    train_y = np.empty((0,1))\n",
    "    lowerB = 0.99\n",
    "    upperB = 1.01\n",
    "    lowerR = 1.2\n",
    "    upperR = 1.5\n",
    "    for i in range(0,num_items):\n",
    "        ref_vector1 = np.array(np.random.uniform(low=0.01, high=0.6, size=size_ref_vector))\n",
    "        for icase in range(0,10):\n",
    "            pos = np.array(label_1_matrix[icase,:])\n",
    "            kk_sign = np.array(label_1_sign[icase,:])\n",
    "            counter = 0\n",
    "            k_adjust = random.uniform(lowerB,upperB)\n",
    "            var_vector1 = copy.deepcopy(ref_vector1)*k_adjust\n",
    "            for j in pos:\n",
    "                kk = random.uniform(lowerR,upperR)  \n",
    "                var_vector1[j] = ref_vector1[j]*kk*kk_sign[counter]\n",
    "                counter = counter + 1\n",
    "            zzz_x = np.concatenate((ref_vector1,var_vector1))   \n",
    "            zzz_y = np.array([1])\n",
    "            train_x = np.append(train_x,[zzz_x],axis=0)\n",
    "            train_y = np.append(train_y,[zzz_y],axis=0)\n",
    "        for icase in range(0,5):\n",
    "            pos = np.array(label_0_matrix[icase,:])\n",
    "            kk_sign = np.array(label_0_sign[icase,:])\n",
    "            counter = 0\n",
    "            k_adjust = random.uniform(lowerB,upperB)\n",
    "            var_vector1 = copy.deepcopy(ref_vector1)*k_adjust\n",
    "            for j in pos:\n",
    "                kk = random.uniform(lowerR,upperR)\n",
    "                var_vector1[j] = ref_vector1[j]*kk*kk_sign[counter]\n",
    "                counter = counter + 1\n",
    "            zzz_x = np.concatenate((ref_vector1,var_vector1))   \n",
    "            zzz_y = np.array([0])\n",
    "            train_x = np.append(train_x,[zzz_x],axis=0)\n",
    "            train_y = np.append(train_y,[zzz_y],axis=0)\n",
    "        for icase in range(0,5):\n",
    "            k_adjust = random.uniform(lowerB,upperB)\n",
    "            var_vector1 = copy.deepcopy(ref_vector1)*k_adjust\n",
    "            zzz_x = np.concatenate((ref_vector1,var_vector1))   \n",
    "            zzz_y = np.array([0])\n",
    "            train_x = np.append(train_x,[zzz_x],axis=0)\n",
    "            train_y = np.append(train_y,[zzz_y],axis=0)\n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_1_matrix.shape:  (10, 80)\n",
      "label_0_matrix.shape:  (10, 20)\n",
      "label_1_sign.shape:  (10, 80)\n",
      "label_0_sign.shape:  (10, 20)\n",
      "Generating the training dataset ...\n",
      "(2000, 2000)\n",
      "(2000, 1)\n"
     ]
    }
   ],
   "source": [
    "size_ref_vector = 1000\n",
    "\n",
    "ref_elements_to_change = int(size_ref_vector*0.08) # 8%\n",
    "var_elements_to_change = int(size_ref_vector*0.02) # 2%\n",
    "\n",
    "label_1_matrix = np.random.randint(0,size_ref_vector-1, size=(10,ref_elements_to_change) )\n",
    "label_0_matrix = np.random.randint(0,size_ref_vector-1, size=(10,var_elements_to_change) )\n",
    "\n",
    "label_1_sign = np.random.choice([-1,1], size=(10,ref_elements_to_change) )\n",
    "label_0_sign = np.random.choice([-1,1], size=(10,var_elements_to_change) )\n",
    "\n",
    "print('label_1_matrix.shape: ' , label_1_matrix.shape)\n",
    "print('label_0_matrix.shape: ' , label_0_matrix.shape)\n",
    "print('label_1_sign.shape: ' , label_1_sign.shape)\n",
    "print('label_0_sign.shape: ' , label_0_sign.shape)\n",
    "\n",
    "Nset = 100\n",
    "print('Generating the training dataset ...')\n",
    "train_x, train_y=generate_dataset_new2(Nset,size_ref_vector,label_1_matrix,label_0_matrix,label_1_sign,label_0_sign)\n",
    "print(train_x.shape) # (2000, 2000)\n",
    "print(train_y.shape) # (2000,)\n",
    "\n",
    "# array([[1.],\n",
    "#        [1.],\n",
    "#        [1.],\n",
    "#        ...,\n",
    "#        [0.],\n",
    "#        [0.],\n",
    "#        [0.]])\n",
    "\n",
    "# >>> train_x\n",
    "# array([[ 0.28908019,  0.59879076,  0.17052536, ...,  0.58650623,\n",
    "#         -0.24925832,  0.54755763],\n",
    "#        [ 0.28908019,  0.59879076,  0.17052536, ...,  0.58685261,\n",
    "#          0.20274144,  0.54788101],\n",
    "#        [ 0.28908019,  0.59879076,  0.17052536, ...,  0.58446367,\n",
    "#          0.20191613,  0.54565071],\n",
    "#        ...,\n",
    "#        [ 0.32940272,  0.03123094,  0.22965965, ...,  0.22620247,\n",
    "#          0.0247488 ,  0.29854916],\n",
    "#        [ 0.32940272,  0.03123094,  0.22965965, ...,  0.22705594,\n",
    "#          0.02484218,  0.29967561],\n",
    "#        [ 0.32940272,  0.03123094,  0.22965965, ...,  0.22548236,\n",
    "#          0.02467001,  0.29759874]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the testing dataset ...\n",
      "(2000, 2000)\n",
      "(2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Each set of dataset is Nset x 80\n",
    "print('Generating the testing dataset ...')\n",
    "test_x, test_y=generate_dataset_new2(100,size_ref_vector,label_1_matrix,label_0_matrix,label_1_sign,label_0_sign)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "27/27 - 0s - loss: 0.2007 - accuracy: 0.9529 - val_loss: 0.0563 - val_accuracy: 0.9633\n",
      "Epoch 2/50\n",
      "27/27 - 0s - loss: 0.0159 - accuracy: 0.9953 - val_loss: 0.0386 - val_accuracy: 0.9833\n",
      "Epoch 3/50\n",
      "27/27 - 0s - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.0170 - val_accuracy: 0.9900\n",
      "Epoch 4/50\n",
      "27/27 - 0s - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "27/27 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "27/27 - 0s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "27/27 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "27/27 - 0s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "27/27 - 0s - loss: 8.9974e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "27/27 - 0s - loss: 9.2048e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "27/27 - 0s - loss: 6.1725e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "27/27 - 0s - loss: 9.9320e-04 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "27/27 - 0s - loss: 4.0659e-04 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "27/27 - 0s - loss: 5.8089e-04 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "27/27 - 0s - loss: 6.4311e-04 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "27/27 - 0s - loss: 4.5341e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "27/27 - 0s - loss: 3.5419e-04 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "27/27 - 0s - loss: 4.2513e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "27/27 - 0s - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "27/27 - 0s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "27/27 - 0s - loss: 5.0282e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "27/27 - 0s - loss: 8.1206e-04 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "27/27 - 0s - loss: 4.7193e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "27/27 - 0s - loss: 0.0011 - accuracy: 0.9994 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "27/27 - 0s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "27/27 - 0s - loss: 4.4972e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "27/27 - 0s - loss: 4.7314e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "27/27 - 0s - loss: 7.2707e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "27/27 - 0s - loss: 3.2011e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "27/27 - 0s - loss: 3.4067e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "27/27 - 0s - loss: 2.5549e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "27/27 - 0s - loss: 1.7268e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "27/27 - 0s - loss: 1.6563e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "27/27 - 0s - loss: 1.8525e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "27/27 - 0s - loss: 8.7184e-04 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "27/27 - 0s - loss: 3.1442e-04 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "27/27 - 0s - loss: 2.0881e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "27/27 - 0s - loss: 2.3295e-04 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "27/27 - 0s - loss: 2.0882e-04 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "27/27 - 0s - loss: 2.0373e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "27/27 - 0s - loss: 1.7536e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "27/27 - 0s - loss: 2.0802e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "27/27 - 0s - loss: 1.5024e-04 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "27/27 - 0s - loss: 2.2349e-04 - accuracy: 1.0000 - val_loss: 9.9363e-04 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "27/27 - 0s - loss: 2.0882e-04 - accuracy: 1.0000 - val_loss: 9.6636e-04 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "27/27 - 0s - loss: 1.8597e-04 - accuracy: 1.0000 - val_loss: 9.8761e-04 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "27/27 - 0s - loss: 1.7167e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "27/27 - 0s - loss: 0.0013 - accuracy: 0.9988 - val_loss: 8.0659e-04 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "27/27 - 0s - loss: 1.4439e-04 - accuracy: 1.0000 - val_loss: 9.0150e-04 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "27/27 - 0s - loss: 0.0016 - accuracy: 0.9994 - val_loss: 8.3981e-04 - val_accuracy: 1.0000\n",
      "Execution time:  9.251736402511597\n",
      "63/63 - 0s - loss: 0.0604 - accuracy: 0.9865\n",
      "Testing Accuracy =  [0.06039585545659065, 0.9865000247955322]\n"
     ]
    }
   ],
   "source": [
    "# Create ANN model\n",
    "\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_dim = 2000))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid')) # Output layer for binary classification\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def baseline_model4():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu', input_dim = 1000))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid')) # Output layer for binary classification\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "ANN_model = baseline_model()\n",
    "ANN_model.summary()\n",
    "\n",
    "start_time = time.time()\n",
    "history = ANN_model.fit(train_x, train_y, epochs=50, batch_size=64,validation_split=0.15,verbose = 2)\n",
    "end_time = time.time()\n",
    "exe_time = end_time - start_time\n",
    "print(\"Execution time: \", exe_time)\n",
    "scores = ANN_model.evaluate(test_x,test_y,verbose = 2)\n",
    "print(\"Testing Accuracy = \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2000)\n",
      "(2000, 1)\n",
      "(2000, 1)\n",
      "** CHECK Average BCE Loss for multiple samples: [nan]\n",
      "Accuracy Score: 0.9975\n",
      "error0to1 =  4 ; label is 0\n",
      "error1to0 =  1 ; label is 1\n",
      "Testing total error =  5 percentError =  0.25\n",
      "2000 test cases: 1000 label 0; 1000 label 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gpang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: divide by zero encountered in log\n",
      "c:\\users\\gpang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:30: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "# To get the results of the ANN using test dataset\n",
    "y_pred = ANN_model.predict(test_x)\n",
    "\n",
    "epsilon = 1e-15  # Small value to prevent log(0)\n",
    "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "print(y_pred.shape)\n",
    "total_loss = binary_cross_entropy_check(test_y, y_pred)\n",
    "print(f\"** CHECK Average BCE Loss for multiple samples: {total_loss}\")\n",
    "\n",
    "# To check on accuracy\n",
    "# first, convert the elements in y_pred so that negative becones epsilon, largest is 1 - epsilon\n",
    "epsilon = 1e-15  # Small value to prevent log(0)\n",
    "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "N=len(test_y)\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(test_y,y_pred_binary)\n",
    "print(\"Accuracy Score:\",accuracy)\n",
    "\n",
    "#Count error cases:\n",
    "test_y_binary = test_y\n",
    "error0to1 = 0  # test_y is 0\n",
    "error1to0 = 0  # test_y is 1\n",
    "for i in range(N):\n",
    "    if (test_y_binary[i] == 0 and y_pred_binary[i] == 1):\n",
    "        error0to1 += 1\n",
    "    if (test_y_binary[i] == 1 and y_pred_binary[i] == 0):\n",
    "        error1to0 += 1\n",
    "print('error0to1 = ',error0to1, '; label is 0')\n",
    "print('error1to0 = ',error1to0, '; label is 1')\n",
    "print('Testing total error = ',error0to1+error1to0,'percentError = ', 100*(error0to1+error1to0)/N)\n",
    "print('2000 test cases: 1000 label 0; 1000 label 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_x.shape  (2000, 2000)\n",
      "test_y.shape  (2000, 1)\n",
      "(2000, 1)\n",
      "Compare between test_y and y_pred_binary\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0]\n",
      "Sum of errors =  5   Percent Error 0.25\n"
     ]
    }
   ],
   "source": [
    "print('test_x.shape ', test_x.shape)\n",
    "print('test_y.shape ',test_y.shape)\n",
    "print(y_pred_binary.shape)\n",
    "print('Compare between test_y and y_pred_binary')\n",
    "\n",
    "N=len(test_y)\n",
    "errorcase=[0]*20\n",
    "for i in range(0,len(test_y)):\n",
    "    index = (i % 20)\n",
    "    if (test_y[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "print(errorcase)\n",
    "print('Sum of errors = ' ,np.sum(errorcase), '  Percent Error', 100*np.sum(errorcase)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape  (2000, 2000)\n",
      "train_y.shape  (2000, 1)\n",
      "(2000, 1)\n",
      "Compare between train_y and y_pred_binary\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "len(error)  20\n",
      "Sum of errors =  0   Percent Error 0.0\n"
     ]
    }
   ],
   "source": [
    "print('train_x.shape ', train_x.shape)\n",
    "print('train_y.shape ',train_y.shape)\n",
    "\n",
    "# To get the results of the ANN using test dataset\n",
    "y_pred = ANN_model.predict(train_x)\n",
    "\n",
    "epsilon = 1e-15  # Small value to prevent log(0)\n",
    "y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "N=len(train_y)\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "print(y_pred_binary.shape)\n",
    "print('Compare between train_y and y_pred_binary')\n",
    "\n",
    "errorcase=[0]*20\n",
    "for i in range(0,len(train_y)):\n",
    "    index = (i % 20)\n",
    "    if (train_y[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "print(errorcase)\n",
    "print('len(error) '  ,len(errorcase))\n",
    "\n",
    "print('Sum of errors = ' ,np.sum(errorcase), '  Percent Error', 100*np.sum(errorcase)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1000) (2000, 1000) (2000, 1000)\n",
      "(2000, 1000) (2000, 1000) (2000, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Part B:\n",
    "# Difference vector\n",
    "# Getting the difference vector of the training cases for training\n",
    "# input is now 1 x 1000; binary output\n",
    "#\n",
    "ref_vector = train_x[:,0:1000]\n",
    "var_vector = train_x[:,1000:2000]\n",
    "diff_vector = np.abs(ref_vector - var_vector)\n",
    "print(ref_vector.shape,var_vector.shape,diff_vector.shape  )\n",
    "\n",
    "ref_vector_test = test_x[:,0:1000]\n",
    "var_vector_test = test_x[:,1000:2000]\n",
    "diff_vector_test = np.abs(ref_vector_test - var_vector_test)\n",
    "print(ref_vector_test.shape,var_vector_test.shape,diff_vector_test.shape  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN model of the 1000 inputs\n",
    "def baseline_model4():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu', input_dim = 1000))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid')) # Output layer for binary classification\n",
    "    \n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "27/27 - 1s - loss: 0.2219 - accuracy: 0.9265 - val_loss: 0.4861 - val_accuracy: 0.9067\n",
      "Epoch 2/30\n",
      "27/27 - 0s - loss: 0.0441 - accuracy: 1.0000 - val_loss: 0.4150 - val_accuracy: 0.8933\n",
      "Epoch 3/30\n",
      "27/27 - 0s - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.3699 - val_accuracy: 0.9100\n",
      "Epoch 4/30\n",
      "27/27 - 0s - loss: 0.0206 - accuracy: 1.0000 - val_loss: 0.3221 - val_accuracy: 0.9833\n",
      "Epoch 5/30\n",
      "27/27 - 0s - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.2650 - val_accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "27/27 - 0s - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.1992 - val_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "27/27 - 0s - loss: 0.0109 - accuracy: 0.9994 - val_loss: 0.1342 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "27/27 - 0s - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0907 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "27/27 - 0s - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0585 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "27/27 - 0s - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "27/27 - 0s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0240 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "27/27 - 0s - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "27/27 - 0s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "27/27 - 0s - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "27/27 - 0s - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0043 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "27/27 - 0s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "27/27 - 0s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "27/27 - 0s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "27/27 - 0s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "27/27 - 0s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "27/27 - 0s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "27/27 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 8.6797e-04 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "27/27 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 7.7151e-04 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "27/27 - 0s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 6.7471e-04 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "27/27 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 6.1613e-04 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "27/27 - 0s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 5.5196e-04 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "27/27 - 0s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 5.0776e-04 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "27/27 - 0s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.7196e-04 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "27/27 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.2452e-04 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "27/27 - 0s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.9558e-04 - val_accuracy: 1.0000\n",
      "63/63 - 0s - loss: 3.8846e-04 - accuracy: 1.0000\n",
      "Testing Accuracy =  [0.0003884606994688511, 1.0]\n"
     ]
    }
   ],
   "source": [
    "ANN_model_diff = baseline_model4()\n",
    "\n",
    "history = ANN_model_diff.fit(diff_vector, train_y, epochs=30, batch_size=64,validation_split=0.15,verbose = 2)\n",
    "print('Training completed. ')\n",
    "scores = ANN_model_diff.evaluate(diff_vector_test,test_y,verbose = 2)\n",
    "print(\"Testing Accuracy = \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 - 0s - loss: 3.8846e-04 - accuracy: 1.0000\n",
      "0.0003884606994688511 1.0\n"
     ]
    }
   ],
   "source": [
    "# Results based on the vector difference of the 2000 test cases\n",
    "y_pred = ANN_model_diff.predict(diff_vector_test)\n",
    "loss_diff, acc_diff = ANN_model_diff.evaluate(diff_vector_test,test_y,verbose = 2)\n",
    "print(loss_diff, acc_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff_vector_test.shape  (2000, 1000)\n",
      "test_y.shape  (2000, 1)\n",
      "(2000, 1)\n",
      "Compare between test_y and y_pred_binary\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sum of errors =  0   Percent Error 0.0\n"
     ]
    }
   ],
   "source": [
    "print('diff_vector_test.shape ', diff_vector_test.shape)\n",
    "print('test_y.shape ',test_y.shape)\n",
    "\n",
    "y_pred = ANN_model_diff.predict(diff_vector_test)\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "\n",
    "print(y_pred_binary.shape)\n",
    "\n",
    "print('Compare between test_y and y_pred_binary')\n",
    "\n",
    "N=len(test_y)\n",
    "\n",
    "errorcase=[0]*20\n",
    "for i in range(0,len(test_y)):\n",
    "    index = (i % 20)\n",
    "    if (test_y[i] != y_pred_binary[i]): \n",
    "        errorcase[index] = errorcase[index]+1\n",
    "print(errorcase)\n",
    "print('Sum of errors = ' ,np.sum(errorcase), '  Percent Error', 100*np.sum(errorcase)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/gpang/iCloudDrive/Naja/20250628-Evo2/code\\trainX_vectors3.csv\n",
      "Finished Export of Training cases \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "DIR = \"C:/Users/gpang/iCloudDrive/Naja/20250628-Evo2/code\"\n",
    "out_path = os.path.join(DIR, \"trainX_vectors3.csv\")\n",
    "print(out_path)\n",
    "# DB_train.tofile(out_path,sep=',',fmt='%.5f')\n",
    "np.savetxt(out_path,DBset,delimiter=',',fmt='%.5f')\n",
    "print(\"Finished Export of Training cases \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/gpang/iCloudDrive/Naja/20250628-Evo2/code\\trainY_vectors3.csv\n",
      "Finished Export of Testing cases \n"
     ]
    }
   ],
   "source": [
    "out_path = os.path.join(DIR, \"trainY_vectors3.csv\")\n",
    "print(out_path)\n",
    "# DB_test.tofile(out_path,sep=',',fmt='#.5f')\n",
    "np.savetxt(out_path,DBy,delimiter=',',fmt='%.5f')\n",
    "print(\"Finished Export of Testing cases \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/gpang/iCloudDrive/Naja/20250628-Evo2/code\\testX_vectors3.csv\n",
      "Finished Export of Training cases \n"
     ]
    }
   ],
   "source": [
    "out_path = os.path.join(DIR, \"testX_vectors3.csv\")\n",
    "print(out_path)\n",
    "# DB_train.tofile(out_path,sep=',',fmt='%.5f')\n",
    "np.savetxt(out_path,DBset_test,delimiter=',',fmt='%.5f')\n",
    "print(\"Finished Export of Training cases \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO USE BELOW\n",
    "\n",
    "#Define ANOTHER ANN model\n",
    "input_dim = 16384\n",
    "ANNmodel2 = keras.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=(1500,input_dim)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid') # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# compile with cross entropy\n",
    "ANNmodel2.compile(optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = ANNmodel2.fit(x_train, y_train, epochs=30, batch_size=32,verbose = 2)\n",
    "\n",
    "end_time = time.time()\n",
    "exe_time = end_time - start_time\n",
    "print(\"Execution time: \", exe_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ANNmodel2.evaluate(test_x,test_y,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Binary Cross-Entropy Loss (function): 0.20273661557656092\n",
      "--Average BCE Loss for multiple samples: 0.20273661557656092\n"
     ]
    }
   ],
   "source": [
    "# test code\n",
    "import numpy as np\n",
    "from keras.losses import binary_crossentropy\n",
    "import keras.backend as K\n",
    "\n",
    "# Example true labels and predicted probabilities\n",
    "y_true = np.array([0, 1, 1, 0, 1])\n",
    "y_pred = np.array([0.1, 0.9, 0.8, 0.2, 0.7])\n",
    "\n",
    "# Compute Binary Cross-Entropy using NumPy\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    bce = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return bce\n",
    "\n",
    "bce_loss = binary_cross_entropy(y_true, y_pred)\n",
    "print(f\"***Binary Cross-Entropy Loss (function): {bce_loss}\")\n",
    "\n",
    "#===========================================================================\n",
    "def binary_cross_entropy_np(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates Binary Cross-Entropy loss for multiple samples using NumPy.\n",
    "    y_true: NumPy array of actual labels (0s and 1s)\n",
    "    y_pred: NumPy array of predicted probabilities (between 0 and 1)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # Small value to prevent log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "total_loss = binary_cross_entropy_np(y_true, y_pred)\n",
    "print(f\"--Average BCE Loss for multiple samples: {total_loss}\")\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "# Compute Binary Cross-Entropy using Keras\n",
    "# DOES NOT WORK\n",
    "# bce_loss_keras = binary_crossentropy(K.constant(y_true), K.constant(y_pred)).numpy()\n",
    "# print(f\"Binary Cross-Entropy Loss (Keras): {bce_loss_keras}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4\n",
      "-2.5\n"
     ]
    }
   ],
   "source": [
    "aaa = np.array([0, 1, 1, -2.5, 1 , 3.4])\n",
    "bbb = np.array([0.1, 0.9, 0.8, 0.2, 0.7])\n",
    "\n",
    "print(max(aaa))\n",
    "print(min(aaa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    DBset = np.empty((0,4))\n",
    "    DBset_tmp = np.array([1,2,3,4])\n",
    "    DBset = np.append(DBset,[DBset_tmp],axis=0)\n",
    "    print(DBset)\n",
    "    \n",
    "    DBset_tmp = np.array([5,6,7,8])\n",
    "    DBset = np.append(DBset,[DBset_tmp],axis=0)\n",
    "\n",
    "    print(DBset)\n",
    "    print('========================')\n",
    "    DBset_tmp = np.array([9,10,11,12])\n",
    "    DBset = np.append(DBset,[DBset_tmp],axis=0)\n",
    "\n",
    "    print(DBset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO LOAD / READIN\n",
    "\n",
    "# READ IN CSV (takes 30 seconds)\n",
    "new_train_values = np.loadtxt(\"train_vectors2.csv\",delimiter=\",\")\n",
    "print(new_train_values.shape)\n",
    "new_train = new_train_values.reshape((4000,16385))\n",
    "print('new_train.shape = ',new_train.shape)\n",
    "\n",
    "\n",
    "new_test_values = np.loadtxt(\"test_vectors2.csv\",delimiter=\",\")\n",
    "print(new_test_values.shape)\n",
    "new_test = new_test_values.reshape((4000,16385))\n",
    "print('new_test.shape = ',new_test.shape)\n",
    "\n",
    "train_x = new_train[:,0:16384] \n",
    "train_y = new_train[:,16384]\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "#===================\n",
    "test_x = new_test[:,0:16384] \n",
    "test_y = new_test[:,16384]\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
