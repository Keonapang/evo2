{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRCA1 supervised classification using ANN \n",
    "\n",
    "Date: July 22, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN Training\n",
    "import sys\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGION = \"BRCA1_DATA\" # BRCA1_DATA, RovHer_BRCA1 or RovHer_LDLR, \"both\" (BRCA1 + LDLR RVs)\n",
    "# LAYER=\"blocks.28.mlp.l3\"\n",
    "# COMBO=\"refvar\" # delta, refvar\n",
    "# y_label=\"clinvar\" # clinvar (0, 0.25, 0.5, 0.75,1); class (LOF, FUNC/INT)\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Evo2 embeddings\")\n",
    "parser.add_argument(\"--REGION\", type=str, required=True, help=\"BRCA1_DATA, RovHer_BRCA1 or RovHer_LDLR, both (BRCA1 + LDLR RVs)\")\n",
    "parser.add_argument(\"--LAYER\", required=True,type=str, help=\"embedding layer\")\n",
    "parser.add_argument(\"--COMBO\", required=True,type=str, help=\"delta, refvar\")\n",
    "parser.add_argument(\"--Y_LABEL\", type=str, help=\"clinvar (0, 0.25, 0.5, 0.75,1); class (LOF, FUNC/INT)\")\n",
    "parser.add_argument(\"--SUBSET_METHOD\", type=str, help=\"random, top, bottom, balanced, all\")\n",
    "parser.add_argument(\"--MODEL_SIZE\", type=str, help=\"7B or 40B\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "MODEL_SIZE = args.MODEL_SIZE\n",
    "SUBSET_METHOD = args.SUBSET_METHOD\n",
    "REGION = args.REGION\n",
    "LAYER = args.LAYER\n",
    "COMBO = args.COMBO\n",
    "y_label = args.Y_LABEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set input paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Directories\n",
    "INPUT_DIR = Path(\"/mnt/nfs/rigenenfs/shared_resources/biobanks/UKBIOBANK/pangk/evo2/BRCA1_LDLR\")\n",
    "INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Input data: \n",
    "delta_file = f\"{INPUT_DIR}/{REGION}_{LAYER}_delta.csv\"\n",
    "delta_rev_file = f\"{INPUT_DIR}/{REGION}_{LAYER}_delta_rev.csv\"\n",
    "ref_file = f\"{INPUT_DIR}/{REGION}_{LAYER}_ref.csv\"\n",
    "var_file = f\"{INPUT_DIR}/{REGION}_{LAYER}_var.csv\"\n",
    "ref_rev_file = f\"{INPUT_DIR}/{REGION}_{LAYER}_ref_rev.csv\"\n",
    "var_rev_file = f\"{INPUT_DIR}/{REGION}_{LAYER}_var_rev.csv\"\n",
    "\n",
    "# Embedding input files\n",
    "if REGION == \"BRCA1_DATA\":\n",
    "    file = \"/mnt/nfs/rigenenfs/workspace/pangk/Softwares/evo2/data/BRCA1_DATA.xlsx\" # training variants + labels\n",
    "else:\n",
    "    DIR=\"/mnt/nfs/rigenenfs/shared_resources/biobanks/UKBIOBANK/pangk\"\n",
    "    label_file1 = f\"{DIR}/RARity_monogenic_benchmark/BRCAexchange/BRCA1_clinvar_cleaned.txt\" \n",
    "    label_file2 = f\"{DIR}/RARity_monogenic_benchmark/LOVD_LDLR/LDLR_clinvar_curated.txt\" # British heart foundation-classified variants on LOVD\n",
    "\n",
    "if REGION == \"both\":\n",
    "    REGION = \"RovHer_BRCA1\" \n",
    "    ref_file1 = f\"{INPUT_DIR}/{REGION}_{LAYER}_ref.csv\"\n",
    "    var_file1 = f\"{INPUT_DIR}/{REGION}_{LAYER}_var.csv\"\n",
    "    ref_rev_file1 = f\"{INPUT_DIR}/{REGION}_{LAYER}_ref_rev.csv\"\n",
    "    var_rev_file1 = f\"{INPUT_DIR}/{REGION}_{LAYER}_var_rev.csv\"\n",
    "    REGION = \"RovHer_LDLR\" \n",
    "    ref_file2 = f\"{INPUT_DIR}/{REGION}_{LAYER}_ref.csv\"\n",
    "    var_file2 = f\"{INPUT_DIR}/{REGION}_{LAYER}_var.csv\"\n",
    "    ref_rev_file2 = f\"{INPUT_DIR}/{REGION}_{LAYER}_ref_rev.csv\"\n",
    "    var_rev_file2 = f\"{INPUT_DIR}/{REGION}_{LAYER}_var_rev.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set output paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Directories\n",
    "OUTPUT_DIR = Path(f\"/mnt/nfs/rigenenfs/shared_resources/biobanks/UKBIOBANK/pangk/evo2/NN/BRCA1_LDLR_{COMBO}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# OUTPUT FILES\n",
    "plot1 = f\"{OUTPUT_DIR}/{REGION}_{LAYER}_{y_label}_AUC_loss.png\"\n",
    "plot2 = f\"{OUTPUT_DIR}/{REGION}_{LAYER}_{y_label}_ROC.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "def sample_data(df, sample_frac=1.0, balanced=True, disable=True, random_state=42):\n",
    "    \"\"\"Sample dataframe, optionally with balanced classes.\n",
    "    \"\"\"\n",
    "    if disable:\n",
    "        return df\n",
    "    if balanced: # Get the number of rows in the dataframe\n",
    "        num_rows_minor_class = math.ceil(len(df[df[\"class\"] == \"LOF\"]) * sample_frac)\n",
    "        return (\n",
    "            pd.concat(\n",
    "                [\n",
    "                    df[df[\"class\"] == \"LOF\"].sample(n=num_rows_minor_class, random_state=random_state),\n",
    "                    df[df[\"class\"] == \"FUNC/INT\"].sample(n=num_rows_minor_class, random_state=random_state),\n",
    "                ]\n",
    "            )\n",
    "            .sample(frac=1.0, random_state=random_state)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "    else: # Calculate the number of rows to sample\n",
    "        return df.sample(frac=sample_frac, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "def subset_dataframe(df, seq):\n",
    "    \"\"\"\n",
    "    Randomly subsets the dataframe to SEQ_LENGTH number of rows.\n",
    "    Returns: pandas.DataFrame - A subset of the dataframe with SEQ_LENGTH rows.\n",
    "    \"\"\"\n",
    "    print(\"Number of rows to extract:\", seq) \n",
    "    if seq > len(df):\n",
    "        raise ValueError(f\"SEQ_LENGTH ({seq}) is greater than the number of rows in the DataFrame ({len(df)}).\")\n",
    "    subset_df = df.sample(n=seq, random_state=42)\n",
    "    print(\"New subset:\", subset_df.shape) \n",
    "    return subset_df\n",
    "\n",
    "# Compute Binary Cross-Entropy using NumPy\n",
    "def binary_cross_entropy_np(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates Binary Cross-Entropy loss for multiple samples using NumPy.\n",
    "    y_true: NumPy array of actual labels (0s and 1s)\n",
    "    y_pred: NumPy array of predicted probabilities (between 0 and 1)\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15  # Small value to prevent log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # Clip probabilities\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "def recode_clinvar(value):\n",
    "    mapping = {\n",
    "        \"P\": 1,\n",
    "        \"B\": 0,\n",
    "        \"LB\": 0.25,\n",
    "        \"LP\": 0.75,\n",
    "        \"LP,P\": 0.75,\n",
    "        \"B/LB\": 0.25\n",
    "    }\n",
    "    return mapping.get(value, 0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load training data\n",
    "\n",
    "### a) Labels (either `clinvar` or `class`)\n",
    "\n",
    "The *BRCA1* SNV dataset was obtained from [Findlay et al. (2018)](https://www.nature.com/articles/s41586-018-0461-z), which contains 3,893 SNVs. Among them, 631 SNVs have `clinvar` classification [0, 0.25, 0.5, 0.75, 1]. This dataset also contains functional `class` annotations (LOF or FUNC/INT).\n",
    "\n",
    "The *BRCA1* RV dataset was obtained from the RovHer study using UKB WES data, which contains contains over 1,000 RVs, of which 717 RVs have `clinvar` classification [0, 0.25, 0.5, 0.75, 1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recode_map = {\n",
    "    \"Pathogenic\": 1,\n",
    "    'Pathogenic/Likely pathogenic': 1,\n",
    "    'Likely pathogenic': 0.75,\n",
    "    \"Uncertain significance\": 0.5,\n",
    "    \"Likely benign\": 0.25,\n",
    "    \"Benign\": 0,\n",
    "    \"absent\": \"NA\",\n",
    "    'Conflicting interpretations of pathogenicity': \"NA\",\n",
    "}\n",
    "\n",
    "# 1. Variant data + ClinVar labels \n",
    "if REGION == \"BRCA1_DATA\":\n",
    "    data = pd.read_excel(file, header=2)\n",
    "    data = data[['chromosome', 'position (hg19)', 'reference', 'alt', 'function.score.mean', 'func.class', 'clinvar',]]\n",
    "    data.rename(columns={\n",
    "            'chromosome': 'chrom','position (hg19)': 'pos',\n",
    "            'reference': 'ref','alt': 'alt',\n",
    "            'function.score.mean': 'score','func.class': 'class', 'clinvar': 'clinvar',\n",
    "        }, inplace=True)\n",
    "    # Re-code values \n",
    "    data['class'] = data['class'].replace(['FUNC', 'INT'], 'FUNC/INT')\n",
    "    data[\"class\"] = data[\"class\"].replace({0: \"FUNC/INT\", 1: \"LOF\"})\n",
    "    # Create new column \n",
    "    data['PLINK_SNP_NAME'] = data.apply(\n",
    "            lambda row: f\"{row['chrom']}:{row['pos']}:{row['ref']}:{row['alt']}\", axis=1\n",
    "    )\n",
    "    # Recode `clinvar` column\n",
    "    unique_clinvar_values = data['clinvar'].unique()\n",
    "    print(\"Unique values in clinvar column:\", unique_clinvar_values)\n",
    "    data['clinvar'] = data['clinvar'].replace(recode_map)\n",
    "else:\n",
    "    # BRCA1\n",
    "    ACMG_col1 = pd.read_csv(label_file1, sep=\"\\t\", usecols=[\"PLINK_SNP_NAME\", \"ACMG_final\"])\n",
    "    ACMG_col1 = ACMG_col1.rename(columns={\"ACMG_final\": \"clinvar\"})\n",
    "    # LDLR\n",
    "    ACMG_col2 = pd.read_csv(label_file2, sep=\"\\t\", usecols=[\"PLINK_SNP_NAME\", \"clinvar_clnsig\"])\n",
    "    ACMG_col2 = ACMG_col2.rename(columns={\"clinvar_clnsig\": \"clinvar\"})\n",
    "    # Combine \n",
    "    data = pd.concat([ACMG_col1, ACMG_col2], ignore_index=True)\n",
    "    print(f\"BRCA1 and LDLR merged: {data.shape}\")\n",
    "    # (883, 2)\n",
    "    data = data[~data[\"clinvar\"].isin([\"\", \"NA\", \"CCP\"])]\n",
    "    data[\"clinvar\"] = data[\"clinvar\"].apply(recode_clinvar)\n",
    "    print(data[\"clinvar\"].value_counts(dropna=False))\n",
    "\n",
    "# Remove rows with missing clinvar anno\n",
    "data = data[data['clinvar'] != \"NA\"]\n",
    "print(\"After removing NA in clinvar:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Evo2 7B emeddings\n",
    "\n",
    "* `delta`: delta + delta reverse complement embeddings concatenated (8192-dimensional)\n",
    "\n",
    "* `refvar`: ref + ref reverse complement + var + var reverse complement concatenated (16384-dimensional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMBO == \"delta\":\n",
    "    # Variant + reverse complement embeddings \n",
    "    delta = pd.read_csv(delta_file)\n",
    "    delta_reverse = pd.read_csv(delta_rev_file)\n",
    "\n",
    "if COMBO == \"refvar\":\n",
    "    if REGION == \"BRCA1_DATA\" or REGION == \"RovHer_BRCA1\" or REGION == \"RovHer_LDLR\":\n",
    "        # 1. Variant + reverse complement \n",
    "        var = pd.read_csv(var_file)\n",
    "        var_reverse = pd.read_csv(var_rev_file)\n",
    "        # 2. Reference + reverse complement\n",
    "        ref = pd.read_csv(ref_file)\n",
    "        ref_reverse = pd.read_csv(ref_rev_file)\n",
    "\n",
    "    if REGION == \"both\":\n",
    "        var1 = pd.read_csv(var_file1)\n",
    "        var_reverse1 = pd.read_csv(var_rev_file1)\n",
    "        ref1 = pd.read_csv(ref_file1)\n",
    "        ref_reverse1 = pd.read_csv(ref_rev_file1)\n",
    "        var2 = pd.read_csv(var_file2)\n",
    "        var_reverse2 = pd.read_csv(var_rev_file2)\n",
    "        ref2 = pd.read_csv(ref_file2)\n",
    "        ref_reverse2 = pd.read_csv(ref_rev_file2)\n",
    "\n",
    "        var = pd.concat([var1, var2], ignore_index=True)\n",
    "        var_reverse = pd.concat([var_reverse1, var_reverse2], ignore_index=True)\n",
    "        ref = pd.concat([ref1, ref2], ignore_index=True)\n",
    "        ref_reverse = pd.concat([ref_reverse1, ref_reverse2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "### a) Subset rows from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows based on the PLINK_SNP_NAME column\n",
    "data = data[~data['PLINK_SNP_NAME'].duplicated(keep='first')]\n",
    "\n",
    "if COMBO == \"delta\":\n",
    "    # Step 1: Compute the strict intersection of PLINK_SNP_NAME across all dfs\n",
    "    final_common_snp_names = list(\n",
    "        set(data['PLINK_SNP_NAME'])\n",
    "        .intersection(delta['PLINK_SNP_NAME'])\n",
    "        .intersection(delta_reverse['PLINK_SNP_NAME'])\n",
    "    )\n",
    "    # Step 2: Filter all dfs simultaneously based on the common SNP names\n",
    "    data = data[data['PLINK_SNP_NAME'].isin(final_common_snp_names)].reset_index(drop=True)\n",
    "    delta = delta[delta['PLINK_SNP_NAME'].isin(final_common_snp_names)].reset_index(drop=True)\n",
    "    delta_reverse = delta_reverse[delta_reverse['PLINK_SNP_NAME'].isin(final_common_snp_names)].reset_index(drop=True)\n",
    "    # Tallies\n",
    "    print(\"Filtered labels file (data):\", data.shape)\n",
    "    print(\"delta:\", delta.shape, \"delta_reverse:\", delta_reverse.shape)\n",
    "    # Check if the number of rows match\n",
    "    if not (delta.shape[0] == data.shape[0] and\n",
    "            delta_reverse.shape[0] == data.shape[0]):\n",
    "        raise ValueError(\"Number of rows in embeddings do not match number of rows in data.\")\n",
    "\n",
    "if COMBO == \"refvar\":\n",
    "    # Step 1: Compute the strict intersection of PLINK_SNP_NAME across all dfs\n",
    "    final_common_snp_names = list(\n",
    "        set(data['PLINK_SNP_NAME'])\n",
    "        .intersection(var['PLINK_SNP_NAME'])\n",
    "        .intersection(var_reverse['PLINK_SNP_NAME'])\n",
    "        .intersection(ref['PLINK_SNP_NAME'])\n",
    "        .intersection(ref_reverse['PLINK_SNP_NAME'])\n",
    "    )\n",
    "    # Step 2: Filter all dfs based on the common SNP names\n",
    "    data = data[data['PLINK_SNP_NAME'].isin(final_common_snp_names)].reset_index(drop=True)\n",
    "    var = var[var['PLINK_SNP_NAME'].isin(final_common_snp_names)].reset_index(drop=True)\n",
    "    var_reverse = var_reverse[var_reverse['PLINK_SNP_NAME'].isin(final_common_snp_names)].reset_index(drop=True)\n",
    "    ref = ref[ref['PLINK_SNP_NAME'].isin(final_common_snp_names)].reset_index(drop=True)\n",
    "    ref_reverse = ref_reverse[ref_reverse['PLINK_SNP_NAME'].isin(final_common_snp_names)].reset_index(drop=True)\n",
    "\n",
    "    # Tallies\n",
    "    print(\"Filtered labels file (data):\", data.shape)\n",
    "    print(\"var:\", var.shape, \"var_reverse:\", var_reverse.shape, \"ref:\", ref.shape, \"ref_reverse:\", ref_reverse.shape)\n",
    "\n",
    "    # Check if the number of rows match\n",
    "    if not (var.shape[0] == data.shape[0] and\n",
    "            var_reverse.shape[0] == data.shape[0] and\n",
    "            ref.shape[0] == data.shape[0] and\n",
    "            ref_reverse.shape[0] == data.shape[0]):\n",
    "        raise ValueError(\"Number of rows in embeddings do not match number of rows in data.\")\n",
    "\n",
    "print(f\"---------- Values in {y_label} column -------------\\n\")\n",
    "print(data[y_label].value_counts())\n",
    "numeric_rows = data[y_label].apply(lambda x: isinstance(x, (int, float))).sum()\n",
    "print(\"Number of non-missing labels:\", numeric_rows, \"of\", NROWS, \"rows\") #  631 of 3893 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Remove columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'input_file' and 'layer' columns\n",
    "\n",
    "if COMBO == \"delta\":\n",
    "    delta = delta.drop(columns=['PLINK_SNP_NAME','input_file', 'layer'])\n",
    "    delta_reverse = delta_reverse.drop(columns=['PLINK_SNP_NAME','input_file', 'layer'])\n",
    "    print(f\"Variant embeddings: {delta.shape}\")\n",
    "    print(f\"Variant reverse comp. embeddings: {delta_reverse.shape}\")\n",
    "\n",
    "if COMBO == \"refvar\":\n",
    "    var = var.drop(columns=['PLINK_SNP_NAME','input_file', 'layer'])\n",
    "    var_reverse = var_reverse.drop(columns=['PLINK_SNP_NAME','input_file', 'layer'])\n",
    "    ref = ref.drop(columns=['PLINK_SNP_NAME','input_file', 'layer'])\n",
    "    ref_reverse = ref_reverse.drop(columns=['PLINK_SNP_NAME','input_file', 'layer'])\n",
    "    print(f\"Variant embeddings: {var.shape}\") # (631, 4096)\n",
    "    print(f\"Variant reverse comp. embeddings: {var_reverse.shape}\") # (631, 4096)\n",
    "    print(f\"Reference embeddings: {ref.shape}\") # (631, 4096)\n",
    "    print(f\"Reference reverse comp. embeddings: {ref_reverse.shape}\") # (631, 4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build feature vector by concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMBO == \"delta\":\n",
    "    # feature vector for each SNV  (631, 8192)\n",
    "    feature_vec = np.hstack([\n",
    "        delta.values,         # delta embeddings\n",
    "        delta_reverse.values, # Reverse complement\n",
    "    ])\n",
    "    \n",
    "if COMBO == \"refvar\":\n",
    "    # feature vector for each SNV (3893, 16384) | 16384 features per SNV\n",
    "    feature_vec = np.hstack([\n",
    "        ref.values,         # Reference embeddings\n",
    "        ref_reverse.values, # Reverse complement of reference\n",
    "        var.values,         # Variant embeddings\n",
    "        var_reverse.values  # Reverse complement of variant\n",
    "    ])\n",
    "\n",
    "print(f\"feature_vec embeddings: {feature_vec.shape}\") #  (812, 8194)\n",
    "\n",
    "# Extract y-label vectors\n",
    "print(f\"y labels to extract: {y_label}\\n\")\n",
    "train_y = data[y_label].values\n",
    "if \"class\" in data.columns:\n",
    "    lof_count = data[data[\"class\"] == \"LOF\"].shape[0]\n",
    "    print(f\"Test 'LOF': {lof_count}\\n\")\n",
    "    other_count = data[data[\"class\"] == \"FUNC/INT\"].shape[0]\n",
    "    print(f\"Test 'FUNC/INT': {other_count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset\n",
    "1. Training Set: 80% of the training data\n",
    "2. Test Set: 20% of the data (withheld entirely from training)\n",
    "3. Validation Set: 20% of the remaining training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all of X_train for training (more training data), w/ internal validation_split during training\n",
    "internal_validation_split=\"no\"\n",
    "\n",
    "if internal_validation_split == \"yes\":\n",
    "    # Split dataset into test (20%) and remaining training data (80%)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        feature_vec, train_y, test_size=0.2, random_state=42, stratify=train_y\n",
    "    )\n",
    "    print(f\"Training set size: {X_train.shape}\") \n",
    "else: \n",
    "    # MANUALLY split into test (20%) and remaining training data (80%)\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        feature_vec, train_y, test_size=0.2, random_state=42, stratify=train_y\n",
    "    )\n",
    "    # Split remaining training data into train (80%) and validation (20%)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.2, random_state=42, stratify=y_train_val\n",
    "    )\n",
    "    X_val = X_val.astype('float32')\n",
    "    y_val = y_val.astype('float32')\n",
    "    X_train_val = X_train_val.astype('float32')\n",
    "    y_train_val = y_train_val.astype('float32')\n",
    "    print(f\"Validation set size: {X_val.shape}\")\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\") # (403, 16384)\n",
    "print(f\"Test set size: {X_test.shape}\") # (127, 16384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ANN\n",
    "* Input Layer = 32,768 features.\n",
    "* Hidden Layers: 512 → 128 → 32 neurons.\n",
    "* Output Layer: Binary classification (pathogenic probability).\n",
    "* Activation: ReLU for hidden layers, Sigmoid for the output layer.\n",
    "* Batch Normalization and Dropout (𝑝=0.3) after each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ANN model, with output layer for binary classification\n",
    "input_dim = feature_vec.shape[1]\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu',input_dim=input_dim))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['AUC'])\n",
    "    return model\n",
    "\n",
    "ANN_model = build_model()\n",
    "ANN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# reserves 15% of the training data (X_train and y_train) for validation during training\n",
    "if internal_validation_split == \"yes\":\n",
    "    history = ANN_model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=100, \n",
    "        batch_size=64, \n",
    "        validation_split=0.15, \n",
    "    )\n",
    "    print(\"Internal validation split enabled during training\\n\")\n",
    "else:\n",
    "    history = ANN_model.fit(\n",
    "        X_train, y_train, \n",
    "        epochs=100, \n",
    "        batch_size=64, \n",
    "        verbose=2\n",
    "    )\n",
    "    print(\"No internal validation split occured.\\n\")\n",
    "\n",
    "end_time = time.time()\n",
    "exe_time = end_time - start_time\n",
    "print(\"Training time: \", exe_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test and validation sets\n",
    "If the labels are `clinvar` (0, 0.25, 0.5, 0.75, 1):\n",
    "* AUC evaluates P/LP versus B/LB, with VUS (labels with 0.5) removed\n",
    "\n",
    "If labels are `class` (LOF as 0; FUNC/INT as 1):\n",
    "* AUC evaluates LOF versus FUNC/INT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if y_label == \"clinvar\":\n",
    "    print(f\"\\n------------- AUC (P/LP vs B/LB) ------------\\n\")\n",
    "    \n",
    "    # Remove VUS; which are rows where y_test == 0.5\n",
    "    test_mask = y_test != 0.5\n",
    "    X_test_filtered = X_test[test_mask]\n",
    "    y_test_filtered = y_test[test_mask]\n",
    "    val_mask = y_val != 0.5\n",
    "    X_val_filtered = X_val[val_mask]\n",
    "    y_val_filtered = y_val[val_mask]\n",
    "    \n",
    "    # Recode labels: 0.75 (LP) is recoded to 1, 0.25 (LB) is recoded to 0 \n",
    "    y_test_filtered = np.where(y_test_filtered == 0.75, 1, y_test_filtered) \n",
    "    y_test_filtered = np.where(y_test_filtered == 0.25, 0, y_test_filtered)\n",
    "    print(f\"y_test: {y_test.shape}\")\n",
    "    print(f\"y_test_filtered: {y_test_filtered.shape}\\n\")\n",
    "    y_val_filtered = np.where(y_val_filtered == 0.75, 1, y_val_filtered)\n",
    "    y_val_filtered = np.where(y_val_filtered == 0.25, 0, y_val_filtered)\n",
    "    \n",
    "    # Predict probabilities on the test and validation sets\n",
    "    y_test_pred_prob = ANN_model.predict(X_test_filtered).ravel()\n",
    "    y_val_pred_prob = ANN_model.predict(X_val_filtered).ravel()\n",
    "    \n",
    "    # Calculate AUROC for the test/validation set\n",
    "    auc_test = roc_auc_score(y_test_filtered, y_test_pred_prob)\n",
    "    if internal_validation_split == \"no\":  \n",
    "        auc_val = roc_auc_score(y_val_filtered, y_val_pred_prob)\n",
    "    \n",
    "    # For plotting ROC curve\n",
    "    fpr_test, tpr_test, thresholds_test = roc_curve(y_test_filtered, y_test_pred_prob)\n",
    "    fpr_val, tpr_val, thresholds_val = roc_curve(y_val_filtered, y_val_pred_prob)\n",
    "\n",
    "else:\n",
    "    print(f\"\\n------------- AUC (LOF vs FUNC/INT) ------------\\n\")\n",
    "\n",
    "    y_test_pred_prob = ANN_model.predict(X_test).ravel()\n",
    "    y_val_pred_prob = ANN_model.predict(X_val).ravel()\n",
    "\n",
    "    # Calculate AUROC for the test/validation set\n",
    "    auc_test = roc_auc_score(y_test, y_test_pred_prob)\n",
    "    if internal_validation_split == \"no\":  \n",
    "        auc_val  = roc_auc_score(y_val, y_val_pred_prob)\n",
    "\n",
    "    # For plotting ROC curve\n",
    "    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_pred_prob)\n",
    "    fpr_val, tpr_val, thresholds_val = roc_curve(y_val, y_val_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training loss & AUC\n",
    "* Loss vs Epochs and AUC vs Epochs (Training)\n",
    "* ROC curve (Test and Validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation AUC\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['auc'], label='Training AUC')\n",
    "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
    "plt.title('Training AUC')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/{REGION}_{LAYER}_train_AUC_loss.png\")\n",
    "print(\"Loss plot:\", f\"{OUTPUT_DIR}/{REGION}_{LAYER}_train_AUC_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot test/validation ROC curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Test set\n",
    "plt.plot(fpr_test, tpr_test, label=f\"Test Set AUC = {auc_test:.4f}\", color=\"blue\", linewidth=2)\n",
    "\n",
    "# Validation set\n",
    "plt.plot(fpr_val, tpr_val, label=f\"Validation Set AUC = {auc_val:.4f}\", color=\"green\", linewidth=2)\n",
    "\n",
    "# Random guess line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\", color=\"gray\", linewidth=1.5)\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "plt.title(f\"ROC Curve for {y_label}\", fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "\n",
    "# Grid and layout adjustments\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f\"{OUTPUT_DIR}/{REGION}_{LAYER}_ROC.png\")\n",
    "plt.show()\n",
    "print(f\"AUC (Test): {auc_test:.4f}  (Validation): {auc_val:.4f}\")\n",
    "print(\"Results in:\", f\"{OUTPUT_DIR}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
